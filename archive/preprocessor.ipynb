{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MolmoImageProcessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image processor class for Molmo\"\"\"\n",
    "from typing import List, Optional, Union, Mapping\n",
    "\n",
    "import numpy as np\n",
    "import einops\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "\n",
    "from transformers.image_utils import (\n",
    "    OPENAI_CLIP_MEAN,\n",
    "    OPENAI_CLIP_STD,\n",
    "    ImageInput,\n",
    "    is_valid_image,\n",
    ")\n",
    "from transformers.image_processing_utils import BaseImageProcessor\n",
    "\n",
    "\n",
    "def pad_to_bounding_box(\n",
    "    image, offset_height, offset_width, target_height,\n",
    "    target_width, value=0\n",
    "):\n",
    "    height, width = image.shape[:2]\n",
    "    after_padding_width = target_width - offset_width - width\n",
    "    after_padding_height = target_height - offset_height - height\n",
    "    return np.pad(image, [\n",
    "        [offset_height, after_padding_height],\n",
    "        [offset_width, after_padding_width],\n",
    "        [0, 0]\n",
    "    ], constant_values=value)\n",
    "\n",
    "\n",
    "def normalize_image(image, offset, scale):\n",
    "    image -= np.array(offset, dtype=np.float32)[None, None, :]\n",
    "    image /= np.array(scale, dtype=np.float32)[None, None, :]\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_and_pad(\n",
    "    image,\n",
    "    desired_output_size,\n",
    "    resize_method=\"torch-bilinear\",\n",
    "    pad_value=0,\n",
    "    normalize=True,\n",
    "    image_mean=OPENAI_CLIP_MEAN,\n",
    "    image_std=OPENAI_CLIP_STD,\n",
    "):\n",
    "    desired_height, desired_width = desired_output_size\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Cast into float32 since the training code did this in float32 and it (very rarely) effects\n",
    "    # the results after rounding.\n",
    "    image_scale_y = np.array(desired_height, np.float32) / np.array(height, np.float32)\n",
    "    image_scale_x = np.array(desired_width, np.float32) / np.array(width, np.float32)\n",
    "    image_scale = min(image_scale_x, image_scale_y)\n",
    "    scaled_height = int(np.array(height, np.float32) * image_scale)\n",
    "    scaled_width = int(np.array(width, np.float32) * image_scale)\n",
    "\n",
    "    if resize_method == \"tensorflow\":\n",
    "        # This how the original training code did resizing, it can produce slightly different\n",
    "        # results then using torch resize so we keep it just in case\n",
    "        import tensorflow as tf\n",
    "        image = tf.image.convert_image_dtype(tf.constant(image), dtype=tf.float32)\n",
    "        image = tf.image.resize(\n",
    "            image,\n",
    "            [scaled_height, scaled_width],\n",
    "            method=tf.image.ResizeMethod.BILINEAR,\n",
    "            antialias=True,\n",
    "        )\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        image = image.numpy()\n",
    "    elif resize_method == \"torch-bilinear\":\n",
    "        image = torch.permute(torch.from_numpy(image), [2, 0, 1])\n",
    "        image = convert_image_dtype(image)  # resize in float32 to match the training code\n",
    "        image = torchvision.transforms.Resize(\n",
    "            [scaled_height, scaled_width], InterpolationMode.BILINEAR, antialias=True\n",
    "        )(image)\n",
    "        image = torch.clip(image, 0.0, 1.0)\n",
    "        image = torch.permute(image, [1, 2, 0]).numpy()\n",
    "    else:\n",
    "        raise NotImplementedError(resize_method)\n",
    "\n",
    "    top_pad = (desired_height - scaled_height) // 2\n",
    "    left_pad = (desired_width - scaled_width) // 2\n",
    "    padding = [\n",
    "        [top_pad, desired_height - scaled_height - top_pad],\n",
    "        [left_pad, desired_width - scaled_width - left_pad],\n",
    "        [0, 0]\n",
    "    ]\n",
    "    image_mask = np.pad(np.ones_like(image[:, :, 0], dtype=bool), padding[:2])\n",
    "    image = np.pad(image, padding, constant_values=pad_value)\n",
    "    if normalize:\n",
    "        image = normalize_image(image, offset=image_mean, scale=image_std)\n",
    "    return image, image_mask\n",
    "\n",
    "\n",
    "def select_tiling(h, w, patch_size, max_num_patches):\n",
    "    \"\"\"Decide how best to divide in image of size [w, h] in up to max_num_patches of size patch_size\"\"\"\n",
    "    original_size = np.stack([h, w])  # [1, 2]\n",
    "    original_res = h * w\n",
    "    tilings = []\n",
    "    for i in range(1, max_num_patches+1):\n",
    "        for j in range(1, max_num_patches+1):\n",
    "            if i*j <= max_num_patches:\n",
    "                tilings.append((i, j))\n",
    "    # sort so argmin and argmax favour smaller tilings in the event of a tie\n",
    "    tilings.sort(key=lambda x: (x[0]*x[1], x[0]))\n",
    "    candidate_tilings = np.array(tilings, dtype=np.int32)  # [n_resolutions, 2]\n",
    "    candidate_resolutions = candidate_tilings * patch_size  # [n_resolutions, 2]\n",
    "\n",
    "    # How much we would need to scale the image to fit exactly in each tiling\n",
    "    original_size = np.stack([h, w], dtype=np.float32)  # [1, 2]\n",
    "    required_scale_d = candidate_resolutions.astype(np.float32) / original_size\n",
    "    required_scale = np.min(required_scale_d, axis=-1, keepdims=True)  # [n_resolutions, 1]\n",
    "    if np.all(required_scale < 1):\n",
    "        # We are forced to downscale, so try to minimize the amount of downscaling\n",
    "        ix = np.argmax(required_scale)\n",
    "    else:\n",
    "        # Pick the resolution that required the least upscaling so that it most closely fits the image\n",
    "        required_scale = np.where(required_scale < 1.0, 10e9, required_scale)\n",
    "        ix = np.argmin(required_scale)\n",
    "    return candidate_tilings[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_processing_utils import BaseImageProcessor\n",
    "\n",
    "class MolmoImageProcessor(BaseImageProcessor):\n",
    "    \"\"\"Preprocess images and multi-model inputs\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_crops: int = 12,\n",
    "        overlap_margins: List[int] = (4, 4),\n",
    "        base_image_input_size: List[int] = (336, 336),\n",
    "        image_token_length_w: int = 12,\n",
    "        image_token_length_h: int = 12,\n",
    "        image_patch_size: int = 14,\n",
    "        image_padding_mask: bool = True,\n",
    "        do_normalize: bool = True,\n",
    "        image_mean: Optional[Union[float, List[float]]] = None,\n",
    "        image_std: Optional[Union[float, List[float]]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_crops = max_crops\n",
    "        self.overlap_margins = overlap_margins\n",
    "        self.base_image_input_size = base_image_input_size\n",
    "        self.image_token_length_w = image_token_length_w\n",
    "        self.image_token_length_h = image_token_length_h\n",
    "        self.image_patch_size = image_patch_size\n",
    "        self.image_padding_mask = image_padding_mask\n",
    "        self.do_normalize = do_normalize\n",
    "        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n",
    "        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n",
    "\n",
    "    def image_to_patches_and_tokens(\n",
    "        self,\n",
    "        image: ImageInput, # (h, w, c)\n",
    "        \n",
    "        image_patch_token_id: int,\n",
    "        image_col_token_id: int,\n",
    "        image_start_token_id: int,\n",
    "        image_end_token_id: int,\n",
    "        \n",
    "        max_crops: Optional[int] = None,\n",
    "        overlap_margins: Optional[List[int]] = None,\n",
    "        \n",
    "        base_image_input_size: Optional[Union[int, List[int]]] = None,\n",
    "        image_token_length_w: Optional[int] = None,\n",
    "        image_token_length_h: Optional[int] = None,\n",
    "        image_patch_size: Optional[int] = None,\n",
    "    ):\n",
    "        '''\n",
    "        Returns: ???\n",
    "            patches: (n_crops, n_patches, patch_dim) individual crops, `n_crops` might\n",
    "                     change between images but the other dimension are fixed\n",
    "            tokens: (n_tokens,) int32 tokens, pad tokens indicate where to insert the\n",
    "                                    patch features, might include other special tokens as well\n",
    "            patch_ordering: (n_crops, n_patches) index in `tokens` to put the patch features from the\n",
    "                           crops after pooling, negative values indicates patches features to exclude\n",
    "            padding_mask: (n_crops, n_patches) what percent of each crop is padding, can be None\n",
    "                          if the image mask is not being used.\n",
    "                          \n",
    "        '''\n",
    "        if isinstance(base_image_input_size, int):\n",
    "            base_image_input_size = (base_image_input_size, base_image_input_size)\n",
    "\n",
    "        base_image_input_d = image_patch_size\n",
    "        tokens_per_image = image_token_length_w * image_token_length_h\n",
    "        image_base_patch_w = base_image_input_size[1] // base_image_input_d\n",
    "        image_base_patch_h = base_image_input_size[0] // base_image_input_d\n",
    "\n",
    "        original_image_h, original_image_w = image.shape[:2]\n",
    "        crop_size = base_image_input_size[0]\n",
    "\n",
    "        # Discard this many patches from the (left/top, right/bottom) of crops\n",
    "        left_margin, right_margin = overlap_margins\n",
    "        # left_margin, right_margin = 2, 2\n",
    "        assert left_margin % 2 == 0  # Required for compatibility with 2x2 pooling\n",
    "        total_margin_pixels = base_image_input_d*(right_margin + left_margin)  # pixels removed per dim\n",
    "        crop_patches = base_image_input_size[0] // base_image_input_d  # patches per crop dim\n",
    "        crop_window_patches = crop_patches - (right_margin + left_margin)  # usable patches\n",
    "        crop_window_size = crop_window_patches * base_image_input_d\n",
    "        tiling = select_tiling(\n",
    "            original_image_h - total_margin_pixels,\n",
    "            original_image_w - total_margin_pixels,\n",
    "            crop_window_size,\n",
    "            max_crops\n",
    "        )\n",
    "        src, img_mask = resize_and_pad(\n",
    "            image,\n",
    "            [tiling[0]*crop_window_size+total_margin_pixels, tiling[1]*crop_window_size+total_margin_pixels]\n",
    "        )\n",
    "\n",
    "        # Now we have to split the image into crops, while keeping track of how each patch in the\n",
    "        # each crop should be ordered in the global image, this require a lot of tricky booking\n",
    "        n_crops = tiling[0] * tiling[1]\n",
    "        patches_arr = []\n",
    "        mask_arr = []\n",
    "        patch_ordering_arr = []\n",
    "\n",
    "        # We assume 2x2 pooling, but can allow padding the right/bottom with extra\n",
    "        # patches if the number of patches per side is not even\n",
    "        assert (crop_patches+1)//2 == image_token_length_h\n",
    "        assert (crop_patches+1)//2 == image_token_length_w\n",
    "        on = 0\n",
    "        on_patch = 0\n",
    "        for i in range(tiling[0]):\n",
    "            y0 = i*crop_window_size\n",
    "            if i == 0:\n",
    "                crop_y0 = 0\n",
    "            else:\n",
    "                crop_y0 = left_margin // 2\n",
    "\n",
    "            crop_h = image_base_patch_h - (right_margin + left_margin)\n",
    "            if i == 0:\n",
    "                crop_h += left_margin\n",
    "            if i == (tiling[0]-1):\n",
    "                crop_h += right_margin\n",
    "            for j in range(tiling[1]):\n",
    "                x0 = j*crop_window_size\n",
    "                if j == 0:\n",
    "                    crop_x0 = 0\n",
    "                else:\n",
    "                    crop_x0 = left_margin // 2\n",
    "\n",
    "                crop_w = image_base_patch_w - (right_margin + left_margin)\n",
    "                if j == 0:\n",
    "                    crop_w += left_margin\n",
    "                if j == (tiling[1]-1):\n",
    "                    crop_w += right_margin\n",
    "\n",
    "                pooled_w = (crop_w + 1) // 2\n",
    "                pooled_h = (crop_h + 1) // 2\n",
    "                patch_ordering_arr.append(\n",
    "                    pad_to_bounding_box(\n",
    "                        np.reshape(np.arange(on, on+pooled_h*pooled_w, dtype=np.int32), (pooled_h, pooled_w, 1)),\n",
    "                        crop_y0, crop_x0, image_token_length_h, image_token_length_w, value=-1\n",
    "                    )[:, :, 0]\n",
    "                )\n",
    "                patches_arr.append(src[y0:y0+crop_size, x0:x0+crop_size])\n",
    "                mask_arr.append(img_mask[y0:y0+crop_size, x0:x0+crop_size])\n",
    "\n",
    "                on += pooled_h*pooled_w\n",
    "                on_patch += 1\n",
    "        patches = np.stack(patches_arr)\n",
    "        patch_ordering = np.stack(patch_ordering_arr)\n",
    "        img_mask = np.stack(mask_arr)\n",
    "\n",
    "        # Switch to [n_crops, n_patches, pixels_per_patch] format\n",
    "        image_layout_impatch_w, image_layout_impatch_h = tiling[0], tiling[1]\n",
    "        patches = einops.rearrange(\n",
    "            patches, 'p (h dh) (w dw) c -> p (h w) (dh dw c)',\n",
    "            dh=base_image_input_d,\n",
    "            dw=base_image_input_d,\n",
    "            h=image_base_patch_h,\n",
    "            w=image_base_patch_w\n",
    "        )\n",
    "        img_mask = einops.rearrange(\n",
    "            img_mask, 'p (h dh) (w dw) -> p (h w) (dh dw)',\n",
    "            dh=base_image_input_d,\n",
    "            dw=base_image_input_d,\n",
    "            h=image_base_patch_h,\n",
    "            w=image_base_patch_w\n",
    "        )\n",
    "\n",
    "        img_mask = img_mask.astype(np.float32).mean(axis=-1)\n",
    "        patch_ordering = np.reshape(patch_ordering, [-1])\n",
    "        valid = patch_ordering >= 0\n",
    "\n",
    "        # Transpose order, to get left-to-right order instead of crop-by-crop order\n",
    "        patch_ordering_rh = np.reshape(\n",
    "            patch_ordering,\n",
    "            [tiling[0], tiling[1], image_token_length_h, image_token_length_w]\n",
    "        )\n",
    "        patch_ordering_rh = np.transpose(patch_ordering_rh, [0, 2, 1, 3])\n",
    "        patch_ordering_rh = np.reshape(patch_ordering_rh, [-1])\n",
    "\n",
    "        # The transpose will screw up which patches are masked, project the\n",
    "        # new order into sparse structure of `patch_ordering` to fix this\n",
    "        patch_ordering[valid] = patch_ordering_rh[patch_ordering_rh >= 0]\n",
    "\n",
    "        # Now build the output tokens\n",
    "        h = tiling[0] * crop_window_patches + (right_margin+left_margin)\n",
    "        w = tiling[1] * crop_window_patches + (right_margin+left_margin)\n",
    "        per_row = np.full(\n",
    "            ((w+1)//2,),\n",
    "            image_patch_token_id,\n",
    "        )\n",
    "        per_row = np.concatenate([per_row, [image_col_token_id]], 0)\n",
    "\n",
    "        joint = np.tile(per_row, [(h+1)//2])\n",
    "        joint = [\n",
    "            [image_start_token_id],\n",
    "            joint,\n",
    "            [image_end_token_id]\n",
    "        ]\n",
    "\n",
    "        # Finally do the same for the global image\n",
    "        resized, _ = resize_and_pad(image, base_image_input_size)\n",
    "        resized = einops.rearrange(\n",
    "            resized, '(h dh) (w dw) c -> (h w) (dh dw c)',\n",
    "            dh=base_image_input_d,\n",
    "            dw=base_image_input_d,\n",
    "            h=image_base_patch_h,\n",
    "            w=image_base_patch_w\n",
    "        )\n",
    "        patches = np.concatenate([np.expand_dims(resized, 0), patches], 0)\n",
    "\n",
    "        # Global image goes first, so the order of patches in previous crops gets increased\n",
    "        patch_ordering = np.where(\n",
    "            patch_ordering >= 0,\n",
    "            patch_ordering + tokens_per_image,\n",
    "            -1\n",
    "        )\n",
    "        patch_ordering = np.concatenate([np.arange(0, tokens_per_image), patch_ordering], 0)\n",
    "        per_row = np.full(\n",
    "            (image_token_length_w,),\n",
    "            image_patch_token_id,\n",
    "        )\n",
    "        per_row = np.concatenate([per_row, [image_col_token_id]], 0)\n",
    "        extra_tokens = np.tile(per_row, [image_token_length_h])\n",
    "        joint = [\n",
    "                    [image_start_token_id],\n",
    "                    extra_tokens,\n",
    "                    [image_end_token_id],\n",
    "                ] + joint\n",
    "\n",
    "        joint = np.concatenate(joint, 0)\n",
    "        img_mask = np.pad(img_mask, [[0, 1], [0, 0]], constant_values=-1)\n",
    "        return patches, joint, patch_ordering, img_mask\n",
    "\n",
    "    def build_image_input_idx(\n",
    "        self,\n",
    "        image_tokens: np.ndarray, # (n_tokens,)\n",
    "        patch_order: np.ndarray, # (n_crops, n_patches)\n",
    "        \n",
    "        image_patch_token_id: int,\n",
    "        no_image: Optional[bool] = None,\n",
    "        image_token_length_w: Optional[int] = None,\n",
    "        image_token_length_h: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Converts `patch_order` into a mapping of token_id -> patch_id\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            image_input_idx: (n_crops, n_patches) index in `tokens` to put the patch features from the\n",
    "                           crops after pooling, negative values indicates patches features to exclude\n",
    "        \"\"\"\n",
    "\n",
    "        tokens_per_image = image_token_length_w * image_token_length_h\n",
    "        if no_image is not None and no_image:\n",
    "            return np.zeros((0, tokens_per_image), np.int32) # (0, tokens_per_image)\n",
    "\n",
    "        # Indices to insert the patches\n",
    "        image_input_idx = image_tokens == image_patch_token_id\n",
    "        image_input_idx = np.nonzero(image_input_idx)[0].astype(np.int32)\n",
    "\n",
    "        if patch_order is not None:\n",
    "            n_tokens = image_input_idx.shape[0]\n",
    "            patch_order = np.reshape(patch_order, [-1]) # (n_crops * n_patches)\n",
    "            n_patches = patch_order.shape[0]\n",
    "\n",
    "            valid = patch_order >= 0\n",
    "            n_valid_patches = valid.sum()\n",
    "            assert len(image_input_idx) == n_valid_patches\n",
    "\n",
    "            sorted_patch_ixs = np.zeros([n_tokens], np.int32)\n",
    "            sorted_patch_ixs[patch_order[valid]] = np.arange(n_valid_patches, dtype=np.int32)\n",
    "\n",
    "            # Project the inverted mapping into same sparse structure\n",
    "            sorted_patch_ixs_ex = np.full(np.shape(patch_order), -1)\n",
    "            sorted_patch_ixs_ex[valid] = sorted_patch_ixs\n",
    "\n",
    "            # Do the gather and then re-masked outputs that were masked in `sorted_patch_ixs`\n",
    "            valid = (sorted_patch_ixs_ex >= 0).astype(np.int32)\n",
    "            image_input_idx = image_input_idx[sorted_patch_ixs_ex*valid]\n",
    "            image_input_idx = image_input_idx*valid - 100*(1 - valid)\n",
    "            image_input_idx = np.reshape(image_input_idx, [-1, tokens_per_image])\n",
    "        return image_input_idx\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        image: np.ndarray, # (h, w, c)\n",
    "        image_patch_token_id: int,\n",
    "        image_col_token_id: int,\n",
    "        image_start_token_id: int,\n",
    "        image_end_token_id: int,\n",
    "        max_crops: Optional[int] = None,\n",
    "        overlap_margins: Optional[List[int]] = None,\n",
    "        base_image_input_size: Optional[Union[int, List[int]]] = None,\n",
    "        image_token_length_w: Optional[int] = None,\n",
    "        image_token_length_h: Optional[int] = None,\n",
    "        image_patch_size: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Preprocesses an image\n",
    "\n",
    "        Returns:\n",
    "            crops: (n_crops, n_patches, patch_dim) individual crops, `n_crops` might\n",
    "                   change between images but the other dimension are fixed\n",
    "            tokens: (n_tokens,) int32 tokens, pad tokens indicate where to insert the\n",
    "                                patch features, might include other special tokens as well\n",
    "            image_idx: (n_crops, n_patches) index in `tokens` to put the patch features from the\n",
    "                       crops after pooling, negative values indicates patches features to exclude\n",
    "            padding_mask: (n_crops, n_patches) what percent of each crop is padding, can be None\n",
    "                          if the image mask is not being used.\n",
    "        \"\"\"\n",
    "\n",
    "        max_crops = max_crops or self.max_crops\n",
    "        overlap_margins = overlap_margins or self.overlap_margins\n",
    "        base_image_input_size = base_image_input_size or self.base_image_input_size\n",
    "        image_token_length_w = image_token_length_w or self.image_token_length_w\n",
    "        image_token_length_h = image_token_length_h or self.image_token_length_h\n",
    "        image_patch_size = image_patch_size or self.image_patch_size\n",
    "\n",
    "        # TODO: what is this function doing?\n",
    "        crops, image_tokens, patch_ordering, img_mask = self.image_to_patches_and_tokens(\n",
    "            image, # (h, w, c)\n",
    "            image_patch_token_id,\n",
    "            image_col_token_id,\n",
    "            image_start_token_id,\n",
    "            image_end_token_id,\n",
    "            max_crops,\n",
    "            overlap_margins,\n",
    "            base_image_input_size,\n",
    "            image_token_length_w,\n",
    "            image_token_length_h,\n",
    "            image_patch_size,\n",
    "        )\n",
    "        # crops: (n_crops, n_patches, patch_dim)\n",
    "        # image_tokens: (n_tokens,)\n",
    "        # patch_idx: (n_crops, n_patches)\n",
    "        # img_mask: (n_crops, n_patches)\n",
    "        \n",
    "        \n",
    "        \n",
    "        patch_idx = self.build_image_input_idx( #TODO: what is this function doing?\n",
    "            image_tokens,\n",
    "            patch_ordering,\n",
    "            image_patch_token_id,\n",
    "            image_token_length_w=image_token_length_w,\n",
    "            image_token_length_h=image_token_length_h,\n",
    "        )\n",
    "        return crops, image_tokens, patch_idx, img_mask\n",
    "\n",
    "    def multimodal_preprocess(\n",
    "        self,\n",
    "        images: np.ndarray, # (n_images, h, w, c)\n",
    "        tokens: List[int],\n",
    "        image_idx: np.ndarray, # (n_images,)\n",
    "        sequence_length: int,\n",
    "        image_patch_token_id: int,\n",
    "        image_col_token_id: int,\n",
    "        image_start_token_id: int,\n",
    "        image_end_token_id: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Merge images and text tokens into multi-modal features for the model\n",
    "\n",
    "        :param images: images to use as input\n",
    "        :param tokens: input text tokens\n",
    "        :param image_idx: where to insert the images into `tokens`\n",
    "        :params image_patch_token_id: id to use of tokens that will contain image features\n",
    "        :params image_col_token_id: token id for image column special tokens\n",
    "        :params image_start_token_id: token id for image start special tokens\n",
    "        :params image_end_token_id: token id for image end special tokens\n",
    "        :params kwargs: override preprocessor default args\n",
    "        \"\"\"\n",
    "        max_total_crops = kwargs.get(\"max_crops\") or self.max_crops\n",
    "        image_token_length_w = kwargs.get(\"image_token_length_w\") or self.image_token_length_w\n",
    "        image_token_length_h = kwargs.get(\"image_token_length_h\") or self.image_token_length_h\n",
    "        image_patch_size = kwargs.get(\"image_patch_size\") or self.image_patch_size\n",
    "        base_image_input_size = kwargs.get(\"base_image_input_size\") or self.base_image_input_size\n",
    "        image_num_patch = (\n",
    "            base_image_input_size[0] // image_patch_size,\n",
    "            base_image_input_size[1] // image_patch_size,\n",
    "        )\n",
    "        image_padding_mask = kwargs.get(\"image_padding_mask\") or self.image_padding_mask\n",
    "\n",
    "        tokens_per_image = image_token_length_w * image_token_length_h\n",
    "        n_pixels = image_patch_size * image_patch_size * 3\n",
    "        n_patches = image_num_patch[0] * image_num_patch[1]\n",
    "\n",
    "        if images is None:\n",
    "            return {\n",
    "                \"input_ids\": tokens,\n",
    "            }\n",
    "        else:\n",
    "            n = len(images)\n",
    "            all_crops = []\n",
    "            all_image_idx = []\n",
    "            out_tokens = []\n",
    "            all_crop_masks = []\n",
    "\n",
    "            for ix in range(n): # for each image\n",
    "                token_ix = image_idx[ix]\n",
    "                crops, image_tokens, patch_idx, img_mask = self.preprocess( #TODO: what is the output of this function?\n",
    "                    images[ix], # (h, w, c)\n",
    "                    image_patch_token_id,\n",
    "                    image_col_token_id,\n",
    "                    image_start_token_id,\n",
    "                    image_end_token_id,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                \n",
    "                # crops: (n_crops, n_patches, patch_dim) ??\n",
    "                # image_tokens: (n_tokens,) ?? \n",
    "                # patch_idx: (n_crops, n_patches) ??\n",
    "                # img_mask: (n_crops, n_patches) ??\n",
    "\n",
    "                if token_ix == -1:  # -1 is an image inserted at the very start\n",
    "                    start = 0\n",
    "                    token_ix = 0\n",
    "                    end = 0\n",
    "                else:\n",
    "                    start = 0 if ix == 0 else image_idx[ix-1] + 1\n",
    "                    end = token_ix + 1\n",
    "\n",
    "                all_image_idx.append(patch_idx + token_ix)\n",
    "                all_crops.append(crops)\n",
    "                \n",
    "                out_tokens.append(tokens[start:token_ix])\n",
    "                out_tokens.append(image_tokens)\n",
    "                \n",
    "                if ix == (n - 1):\n",
    "                    out_tokens.append(tokens[end:])\n",
    "                if image_padding_mask:\n",
    "                    all_crop_masks.append(img_mask)\n",
    "\n",
    "            input_ids = np.concatenate(out_tokens, 0)\n",
    "            images = np.concatenate(all_crops, 0)\n",
    "            image_input_idx = np.concatenate(all_image_idx, 0)\n",
    "            \n",
    "            if image_padding_mask:\n",
    "                image_masks = np.concatenate(all_crop_masks, 0)\n",
    "            else:\n",
    "                image_masks = None\n",
    "\n",
    "        out = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"images\": images,\n",
    "            \"image_input_idx\": image_input_idx\n",
    "        }\n",
    "        if image_masks is not None:\n",
    "            out[\"image_masks\"] = image_masks\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MolmoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processor class for Molmo.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "# from PIL.Image import Image\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    from typing import Unpack\n",
    "except ImportError:\n",
    "    from typing_extensions import Unpack\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = f\"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = f\"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = f\"<im_end>\"\n",
    "DEFAULT_IM_COL_TOKEN = f\"<im_col>\"\n",
    "IMAGE_PROMPT = \"<|image|>\"\n",
    "\n",
    "EXTRA_TOKENS = (DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_COL_TOKEN, IMAGE_PROMPT)\n",
    "\n",
    "\n",
    "def get_special_token_ids(tokenizer):\n",
    "    ids = tokenizer.encode(\"\".join(EXTRA_TOKENS), add_special_tokens=False)\n",
    "    assert len(ids) == len(EXTRA_TOKENS)\n",
    "    return {k: i for k, i in zip(EXTRA_TOKENS, ids)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MolmoTextKwargs():\n",
    "#     '''\n",
    "#     Default values:\n",
    "    \n",
    "#         {\n",
    "#             \"style\": \"long_caption\",\n",
    "#             \"system_prompt\": \"none\",\n",
    "#             \"message_format\": \"role\",\n",
    "#             \"always_start_with_space\": True,\n",
    "#             \"sequence_length\": 1536,\n",
    "#             \"padding\": False,\n",
    "#         }\n",
    "    \n",
    "#     '''\n",
    "#     style: Optional[str]\n",
    "#     system_prompt: Optional[str]\n",
    "#     message_format: Optional[str]\n",
    "#     always_start_with_space: Optional[bool]\n",
    "#     sequence_length: Optional[int]\n",
    "\n",
    "# class MolmoImagesKwargs():\n",
    "#     '''Loaded from preprocessor_config.json'''\n",
    "#     '''Default values:\n",
    "    \n",
    "#         {\n",
    "#             \"max_crops\": 12,\n",
    "#             \"overlap_margins\": [4, 4],\n",
    "#             \"base_image_input_size\": [336, 336],\n",
    "#             \"image_token_length_w\": 12,\n",
    "#             \"image_token_length_h\": 12,\n",
    "#             \"image_patch_size\": 14,\n",
    "#             \"image_padding_mask\": True,\n",
    "#         },\n",
    "    \n",
    "#     '''\n",
    "#     max_crops: Optional[int]\n",
    "#     overlap_margins: Optional[List[int]]\n",
    "#     base_image_input_size: Optional[List[int]]\n",
    "#     image_token_length_w: Optional[int]\n",
    "#     image_token_length_h: Optional[int]\n",
    "#     image_patch_size: Optional[int]\n",
    "#     image_padding_mask: Optional[bool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolmoProcessor():\n",
    "\n",
    "    def __init__(self, image_processor: MolmoImageProcessor = None, tokenizer : AutoTokenizer = None, **kwargs):\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        # super().__init__(image_processor, tokenizer)\n",
    "        self._special_tokens = None\n",
    "\n",
    "    @property\n",
    "    def special_token_ids(self):\n",
    "        if self._special_tokens is None:\n",
    "            self._special_tokens = get_special_token_ids(self.tokenizer)\n",
    "        return self._special_tokens\n",
    "\n",
    "    def get_tokens_input(self, prompt, message_format, always_start_with_space): #TODO: edit\n",
    "        if message_format == \"none\" or message_format is None:\n",
    "            pass\n",
    "        elif message_format == \"role\": #NOTE: this is run by default\n",
    "            prompt = \"User: \" + prompt + \" Assistant:\"\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Message format {message_format} not implemented\")\n",
    "\n",
    "        if always_start_with_space: #NOTE: this is run by default\n",
    "            prompt = \" \" + prompt\n",
    "\n",
    "        tokens = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        text = None, # str : question # TextInput\n",
    "        images: ImageInput = None, # List of PIL images\n",
    "        *,\n",
    "        tokens = None, # Optional[PreTokenizedInput]\n",
    "        # **kwargs, # Unpack[MolmoProcessorKwargs]\n",
    "        text_kwargs_dict = None,\n",
    "        images_kwargs_dict = None,\n",
    "    ):\n",
    "        # output_kwargs = self._merge_kwargs(\n",
    "        #     MolmoProcessorKwargs,\n",
    "        #     tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n",
    "        #     **kwargs,\n",
    "        # )\n",
    "\n",
    "        if tokens is None:\n",
    "            tokens = self.get_tokens_input( #TODO: edit get_tokens_input to support chat templates\n",
    "                text,\n",
    "                # output_kwargs[\"text_kwargs\"][\"message_format\"],\n",
    "                # output_kwargs[\"text_kwargs\"][\"always_start_with_space\"],\n",
    "                text_kwargs_dict[\"message_format\"],\n",
    "                text_kwargs_dict[\"always_start_with_space\"],\n",
    "            )\n",
    "\n",
    "        image_token_id = self.special_token_ids[IMAGE_PROMPT]\n",
    "\n",
    "        if images is not None:\n",
    "            if not isinstance(images, (list, tuple)):\n",
    "                images = [images]\n",
    "            image_arrays = []\n",
    "            for image in images:\n",
    "                if isinstance(image, Image.Image):\n",
    "                    image = image.convert(\"RGB\")\n",
    "                    # Handle images with EXIF orientation tags, which PIL will ignore by default\n",
    "                    # https://github.com/python-pillow/Pillow/issues/4703\n",
    "                    img = ImageOps.exif_transpose(image)\n",
    "                    image_arrays.append(np.array(image))\n",
    "                else:\n",
    "                    assert len(image.shape) == 3 and image.shape[-1] == 3\n",
    "                    image_arrays.append(image.astype(np.uint8))\n",
    "            \n",
    "            images = image_arrays # list of np arrays # dimensions: (num_images, height, width, channels)\n",
    "            \n",
    "            # For now only support inserting images at the start\n",
    "            image_idx = [-1]*len(images) # ??\n",
    "        else:\n",
    "            image_idx = None\n",
    "\n",
    "        # sequence_length = output_kwargs[\"text_kwargs\"][\"sequence_length\"]\n",
    "        sequence_length = text_kwargs_dict[\"sequence_length\"]\n",
    "\n",
    "        image_patch_token_id = self.special_token_ids[DEFAULT_IMAGE_PATCH_TOKEN]\n",
    "        image_col_token_id = self.special_token_ids[DEFAULT_IM_COL_TOKEN]\n",
    "        image_start_token_id = self.special_token_ids[DEFAULT_IM_START_TOKEN]\n",
    "        image_end_token_id = self.special_token_ids[DEFAULT_IM_END_TOKEN]\n",
    "        \n",
    "        out = self.image_processor.multimodal_preprocess(\n",
    "            images=images, #TODO: edit multimodal_preprocess \n",
    "            image_idx=image_idx, # ??\n",
    "            tokens=np.asarray(tokens).astype(np.int32),\n",
    "            sequence_length=sequence_length,\n",
    "            image_patch_token_id=image_patch_token_id,\n",
    "            image_col_token_id=image_col_token_id,\n",
    "            image_start_token_id=image_start_token_id,\n",
    "            image_end_token_id=image_end_token_id,\n",
    "            # **output_kwargs[\"images_kwargs\"]\n",
    "            **images_kwargs_dict\n",
    "        )\n",
    "\n",
    "        # Prepend BOS\n",
    "        # qwen2 and olmo do not have a BOS, and instead use EOS as a generic seperator token.\n",
    "        bos = self.tokenizer.bos_token_id or self.tokenizer.eos_token_id\n",
    "        decoder_input_tokens = np.pad(out[\"input_ids\"], [[1, 0]], constant_values=bos)\n",
    "        out[\"input_ids\"] = decoder_input_tokens\n",
    "        if \"image_input_idx\" in out:\n",
    "            # Shift patch mapping up by one since we added BOS\n",
    "            image_input_idx = out[\"image_input_idx\"]\n",
    "            out[\"image_input_idx\"] = np.where(image_input_idx < 0, image_input_idx, image_input_idx + 1)\n",
    "\n",
    "        for k, v in out.items():\n",
    "            out[k] = torch.from_numpy(v)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class MolmoImagesKwargs:\n",
    "    \"\"\"\n",
    "    Holds configuration values for image processing.\n",
    "    \"\"\"\n",
    "    max_crops: int = 12\n",
    "    overlap_margins: List[int] = field(default_factory=lambda: [4, 4])\n",
    "    base_image_input_size: List[int] = field(default_factory=lambda: [336, 336])\n",
    "    image_token_length_w: int = 12\n",
    "    image_token_length_h: int = 12\n",
    "    image_patch_size: int = 14\n",
    "    image_padding_mask: bool = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_path: str) -> \"MolmoImagesKwargs\":\n",
    "        \"\"\"\n",
    "        Load configuration from a JSON file and return a MolmoImagesKwargs instance.\n",
    "        Only keys matching the class annotations are used.\n",
    "        \"\"\"\n",
    "        with open(json_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        # Filter out only the keys that are relevant to the class\n",
    "        valid_keys = set(cls.__annotations__.keys())\n",
    "        filtered_config: Dict[str, Any] = {k: v for k, v in config.items() if k in valid_keys}\n",
    "        return cls(**filtered_config)\n",
    "\n",
    "@dataclass\n",
    "class MolmoTextKwargs:\n",
    "    \"\"\"\n",
    "    Holds configuration values for text processing.\n",
    "\n",
    "    Default values:\n",
    "        {\n",
    "            \"style\": \"long_caption\",\n",
    "            \"system_prompt\": \"none\",\n",
    "            \"message_format\": \"role\",\n",
    "            \"always_start_with_space\": True,\n",
    "            \"sequence_length\": 1536,\n",
    "            \"padding\": False,\n",
    "        }\n",
    "    \"\"\"\n",
    "    style: str = \"long_caption\"\n",
    "    system_prompt: str = \"none\"\n",
    "    message_format: str = \"role\"\n",
    "    always_start_with_space: bool = True\n",
    "    sequence_length: int = 1536\n",
    "    padding: bool = False\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_path: str) -> \"MolmoTextKwargs\":\n",
    "        \"\"\"\n",
    "        Load configuration from a JSON file and return a MolmoTextKwargs instance.\n",
    "        Only keys matching the class attributes are used.\n",
    "        \"\"\"\n",
    "        with open(json_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        valid_keys = set(cls.__annotations__.keys())\n",
    "        filtered_config: Dict[str, Any] = {k: v for k, v in config.items() if k in valid_keys}\n",
    "        return cls(**filtered_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kwargs  for image processing\n",
    "images_kwargs_instance = MolmoImagesKwargs.from_json(\"/share/users/shehan/workspace_pointing_lmm/MolmoVideo/molmo_video/hf_configs_Molmo-7B-D-0924/preprocessor_config.json\")\n",
    "images_kwargs_dict = images_kwargs_instance.__dict__\n",
    "\n",
    "# Load kwargs  for text processing\n",
    "text_kwargs_instance = MolmoTextKwargs() #.from_json(\"text_preprocessor_config.json\")\n",
    "text_kwargs_dict = text_kwargs_instance.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_crops': 12, 'overlap_margins': [4, 4], 'base_image_input_size': [336, 336], 'image_token_length_w': 12, 'image_token_length_h': 12, 'image_patch_size': 14, 'image_padding_mask': True}\n"
     ]
    }
   ],
   "source": [
    "print(images_kwargs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'style': 'long_caption', 'system_prompt': 'none', 'message_format': 'role', 'always_start_with_space': True, 'sequence_length': 1536, 'padding': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(text_kwargs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/Molmo-7B-D-0924\")\n",
    "\n",
    "processor = MolmoProcessor(\n",
    "    image_processor=MolmoImageProcessor(**images_kwargs_dict), \n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_path = '../data/torch_datasets/pixmo_images/c0d2bd4805501b23e1cec10ef72a6047267d997bc3d5614892f5827dbb2d173e'\n",
    "# show the image\n",
    "Image.open(image_path)\n",
    "\n",
    "prompt='Describe this image.'\n",
    "\n",
    "inputs = processor.process(\n",
    "            images=[Image.open(image_path)],\n",
    "            # images=[Image.open(image_path), Image.open(image_path)], #TODO\n",
    "            text=prompt,\n",
    "            text_kwargs_dict=text_kwargs_dict,\n",
    "            images_kwargs_dict=images_kwargs_dict,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([151643, 152064, 152066,  ...,     13,  21388,     25]),\n",
       " 'images': tensor([[[-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          ...,\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802]],\n",
       " \n",
       "         [[-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          ...,\n",
       "          [ 1.8865, -0.2513, -0.7550,  ...,  1.9019, -0.1943, -0.7327],\n",
       "          [ 1.8772, -0.2609, -0.7641,  ...,  1.9119, -0.2261, -0.7241],\n",
       "          [ 1.8865, -0.2513, -0.7550,  ...,  1.9303, -0.3710, -0.7925]],\n",
       " \n",
       "         [[-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          ...,\n",
       "          [ 1.5685,  0.7125,  0.2205,  ...,  1.7768,  0.5018, -0.7248],\n",
       "          [ 1.2355,  0.7832,  0.2811,  ...,  1.9097,  1.8764,  1.2966],\n",
       "          [ 1.7787,  1.1752, -0.2208,  ...,  1.5817,  1.5702,  0.5110]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.7118,  1.6246,  1.5360,  ...,  1.7228,  1.7696,  1.6935],\n",
       "          [ 1.8428,  1.9039,  1.8593,  ..., -0.5700,  1.8288, -0.7217],\n",
       "          [-1.5806,  1.1428, -1.3166,  ..., -1.1296,  0.9381, -1.2166],\n",
       "          ...,\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802]],\n",
       " \n",
       "         [[-1.1631, -1.0202, -0.4392,  ..., -1.0252, -1.6447, -1.3198],\n",
       "          [ 1.8991, -0.4917, -0.9628,  ..., -0.5918, -0.6384,  1.5371],\n",
       "          [-0.6006, -0.5227,  1.3889,  ..., -0.6191, -0.4933,  1.4878],\n",
       "          ...,\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802]],\n",
       " \n",
       "         [[-0.8974, -1.3054, -1.1742,  ...,  1.9167, -0.5238, -0.8631],\n",
       "          [ 1.7887, -0.0766, -0.6176,  ...,  1.8922, -0.4884, -0.9267],\n",
       "          [-1.3575, -1.3763, -0.8719,  ..., -0.6392, -1.2547, -1.1230],\n",
       "          ...,\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "          [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802]]]),\n",
       " 'image_input_idx': tensor([[   2,    3,    4,  ...,  154,  155,  156],\n",
       "         [ 160,  161,  162,  ..., -100, -100, -100],\n",
       "         [-100, -100,  170,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., 1176, -100, -100],\n",
       "         [-100, -100, -100,  ..., 1184, -100, -100],\n",
       "         [-100, -100, -100,  ..., 1192, 1193, 1194]], dtype=torch.int32),\n",
       " 'image_masks': tensor([[ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "         ...,\n",
       "         [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
