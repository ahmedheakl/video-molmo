{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['LD_LIBRARY_PATH'] = \"/share/softwares/cuda_cudnn/cuda-12.1/lib64:/lib/x86_64-linux-gnu:\" + os.environ.get('LD_LIBRARY_PATH', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "PIXMO_DATASETS = '/share/users/shehan/workspace_pointing_lmm/data/torch_datasets/pixmo_datasets'\n",
    "\n",
    "pixmo_points_dataset = datasets.load_from_disk( os.path.join(PIXMO_DATASETS, f\"points-pointing\"))[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_url', 'image', 'points', 'count', 'label', 'collection_method'],\n",
       "    num_rows: 127405\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixmo_points_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127405"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pixmo_points_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "ex = pixmo_points_dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_url': 'http://fahum.umsu.ac.id/wp-content/uploads/2019/04/KARATE.jpg',\n",
       " 'image': '../data/torch_datasets/pixmo_images/c0d2bd4805501b23e1cec10ef72a6047267d997bc3d5614892f5827dbb2d173e',\n",
       " 'points': [[{'x': 34.277329420396185, 'y': 20.7860838929869}],\n",
       "  [{'x': 92.09791441294121, 'y': 12.201521414953659},\n",
       "   {'x': 80.82719062114771, 'y': 9.543764275062763},\n",
       "   {'x': 50.87926740295353, 'y': 66.08150706910546},\n",
       "   {'x': 83.88638707892022, 'y': 74.29639277422278}],\n",
       "  [{'x': 17.228106367455783, 'y': 94.59199275157144}],\n",
       "  [{'x': 42.34571938916702, 'y': 11.718292844064406}],\n",
       "  [{'x': 23.346499283000828, 'y': 46.99397851897994},\n",
       "   {'x': 39.447533271277265, 'y': 54.725635653207995},\n",
       "   {'x': 59.734836096505575, 'y': 48.68527851709232},\n",
       "   {'x': 74.70879770560266, 'y': 49.16850708798158}],\n",
       "  [{'x': 20.287302825228306, 'y': 41.67846423919814},\n",
       "   {'x': 34.61722307479433, 'y': 41.67846423919814},\n",
       "   {'x': 54.58250522025712, 'y': 39.020707099307245},\n",
       "   {'x': 71.00555988829908, 'y': 37.57102138663949}],\n",
       "  [{'x': 40.091574630808324, 'y': 28.389678539743663},\n",
       "   {'x': 57.641701678029634, 'y': 20.899635690960228},\n",
       "   {'x': 74.70879770560266, 'y': 28.631292825188286}],\n",
       "  [{'x': 22.54144758358701, 'y': 27.181607112520528}],\n",
       "  [{'x': 23.024478603235302, 'y': 30.322592823300674},\n",
       "   {'x': 40.57460565045661, 'y': 32.49712139230232},\n",
       "   {'x': 57.641701678029634, 'y': 24.523849972629634},\n",
       "   {'x': 74.5477873657199, 'y': 32.013892821413066}],\n",
       "  [{'x': 18.838209766283427, 'y': 66.32312135455008},\n",
       "   {'x': 39.447533271277265, 'y': 47.96043566075844},\n",
       "   {'x': 53.455432841077766, 'y': 44.577835664533666},\n",
       "   {'x': 73.42071498654055, 'y': 60.04114993298979}],\n",
       "  [{'x': 14.973961609097083, 'y': 73.57154991788889},\n",
       "   {'x': 43.31178142846361, 'y': 76.71253562866904},\n",
       "   {'x': 53.455432841077766, 'y': 67.04796421088398},\n",
       "   {'x': 84.20840775868575, 'y': 80.09513562489381}],\n",
       "  [{'x': 20.931344184759364, 'y': 38.05424995752874},\n",
       "   {'x': 40.25258497069109, 'y': 43.61137852275515},\n",
       "   {'x': 57.641701678029634, 'y': 37.32940710119485},\n",
       "   {'x': 75.03081838536818, 'y': 39.74554995564113}],\n",
       "  [{'x': 25.278623361594004, 'y': 82.99450705022934},\n",
       "   {'x': 33.812171375380515, 'y': 85.16903561923098},\n",
       "   {'x': 59.734836096505575, 'y': 77.43737848500292},\n",
       "   {'x': 72.61566328712672, 'y': 85.41064990467562}],\n",
       "  [{'x': 8.050516994138217, 'y': 79.37029276855993}],\n",
       "  [],\n",
       "  [{'x': 23.829530302649125, 'y': 52.309492798761724},\n",
       "   {'x': 39.93056429092556, 'y': 59.074692791211284},\n",
       "   {'x': 54.90452590002264, 'y': 50.61819280064934},\n",
       "   {'x': 78.73405620267177, 'y': 45.7859070917568}]],\n",
       " 'count': [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'label': ['orange background',\n",
       "  'white logo',\n",
       "  'watermark',\n",
       "  'orange lined background',\n",
       "  'person',\n",
       "  'badge',\n",
       "  'hair',\n",
       "  'hijab',\n",
       "  'person smiling',\n",
       "  'jacket',\n",
       "  'purple bag',\n",
       "  'blue lanyard',\n",
       "  'black pants',\n",
       "  'plaquard',\n",
       "  'dog',\n",
       "  'certificate or paper'],\n",
       " 'collection_method': ['pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing',\n",
       "  'pointing']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "data_list = []\n",
    "for label, points in zip(ex[\"label\"], ex[\"points\"]):\n",
    "    data_list.append(dict(\n",
    "        image = Image.open(ex[\"image\"]),\n",
    "        label=label,\n",
    "        points=np.stack([[x[\"x\"] for x in points], [x[\"y\"] for x in points]], -1),\n",
    "        point_scale=100,\n",
    "        style='point_count'\n",
    "    ))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence, Image\n",
    "\n",
    "# Define features for the dataset.\n",
    "features = Features({\n",
    "    'image': Image(),  # Handles PIL images, numpy arrays, or image file paths.\n",
    "    'points': Sequence(feature=Sequence(Value('float32'))),  # Nested sequences for (num_points, 2)\n",
    "    'label': Value('string'),\n",
    "    'point_scale': Value('int64'),\n",
    "    'style': Value('string'),\n",
    "})\n",
    "\n",
    "# data_list is your list where each element is a dict with keys \"image\", \"points\", and \"label\".\n",
    "dataset = Dataset.from_list(data_list, features=features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label', 'points', 'point_scale', 'style'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "# export CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from molmo.model import MolmoForCausalLM\n",
    "from molmo.preprocessor import MolmoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"allenai/Molmo-7B-D-0924\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the processor\n",
    "# from transformers import AutoProcessor\n",
    "# processor = AutoProcessor.from_pretrained(\n",
    "#     model_id,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map='auto',\n",
    "#     torch_dtype='auto'\n",
    "# )\n",
    "\n",
    "processor = MolmoProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c07af77f41d48d9b90829335d61ec14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "USE_QLORA = True\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# load the model\n",
    "if USE_QLORA:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model = MolmoForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "else:\n",
    "    model = MolmoForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,124,736 || all params: 8,032,150,016 || trainable%: 0.1385\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration. By setting target_modules to [\"att_proj\", \"ff_proj\"],\n",
    "# only the transformer (LLM) portion will have LoRA adapters.\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",  # for causal language modeling\n",
    "    r=8,                    # LoRA rank (adjust as needed)\n",
    "    lora_alpha=8,          # scaling factor\n",
    "    lora_dropout=0.1,       # dropout probability for LoRA layers\n",
    "    target_modules=[\"att_proj\", \"ff_proj\"]\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA (this only affects modules matching the target names)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Optional: print out trainable parameters to verify that only the LLM part is modified.\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess function\n",
    "\n",
    "# def process(examples):\n",
    "#     texts = [f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>{example['question']} Answer briefly. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{example['multiple_choice_answer']}<|eot_id|>\" for example in examples]\n",
    "#     images = [[example[\"image\"].convert(\"RGB\")] for example in examples]\n",
    "\n",
    "#     batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "#     labels = batch[\"input_ids\"].clone()\n",
    "#     labels[labels == processor.tokenizer.pad_token_id] = -100 \n",
    "#     labels[labels == 128256] = -100 # image token index\n",
    "#     batch[\"labels\"] = labels\n",
    "#     batch = batch.to(torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(label, points):\n",
    "    # Let's use chat template to format the prompt correctly\n",
    "    # conversation = [\n",
    "    #         {\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": [\n",
    "    #                 {\"type\": \"text\", \"text\": f\"Point to {label}.\"}, #TODO\n",
    "    #                 {\"type\": \"video\"},\n",
    "    #                 ],\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"role\": \"assistant\",\n",
    "    #             \"content\": [\n",
    "    #                 {\"type\": \"text\", \"text\": str(points)}, #TODO\n",
    "    #                  ],\n",
    "    #         },\n",
    "    #     ]\n",
    "    \n",
    "    conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Point to {label}.\" , #TODO\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": str(points), #TODO\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "                conversation, \n",
    "                chat_template=tokenizer.chat_template,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User: Point to car. Assistant: [[34.27732849121094, 20.786083221435547]]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prompt(\"car\", [[34.27732849121094, 20.786083221435547]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages -%}\\n        {%- if (loop.index % 2 == 1 and message['role'] != 'user') or \\n          (loop.index % 2 == 0 and message['role'].lower() != 'assistant') -%}\\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\\n        {%- endif -%}\\n        {{ message['role'].capitalize() + ': ' + message['content'] }}\\n        {%- if not loop.last -%}\\n        {{ ' ' }}\\n        {%- endif %}\\n        {%- endfor -%}\\n        {%- if add_generation_prompt -%}\\n        {{ ' Assistant:' }}\\n        {%- endif %}\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_IMAGE_PATCH_TOKEN = f\"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = f\"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = f\"<im_end>\"\n",
    "DEFAULT_IM_COL_TOKEN = f\"<im_col>\"\n",
    "IMAGE_PROMPT = \"<|image|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(example):\n",
    "    prompt = get_prompt(example[\"label\"], example[\"points\"])\n",
    "    image = example[\"image\"]\n",
    "    \n",
    "    if image:\n",
    "        inputs = processor.process(\n",
    "            images=[image],\n",
    "            text=prompt\n",
    "        )\n",
    "    else:\n",
    "        inputs = processor.process(\n",
    "            text=prompt\n",
    "        )\n",
    "        \n",
    "    # dtype = torch.bfloat16\n",
    "    device = \"cuda\"\n",
    "        \n",
    "    # inputs = inputs.to(torch.bfloat16).to(\"cuda\")\n",
    "    # inputs = {k: v.to(dtype).to(device).unsqueeze(0) for k, v in inputs.items()} # add batch dimension\n",
    "    inputs = {k: v.to(device).unsqueeze(0) for k, v in inputs.items()} # add batch dimension\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "    inputs[\"labels\"][inputs[\"labels\"] == processor.tokenizer.pad_token_id] = -100\n",
    "    # inputs[\"labels\"][inputs[\"labels\"] == processor.special_token_ids[\"<|image|>\"]] = -100 # image token index\n",
    "    inputs[\"labels\"][inputs[\"labels\"] == processor.special_token_ids[IMAGE_PROMPT]] = -100\n",
    "    inputs[\"labels\"][inputs[\"labels\"] == processor.special_token_ids[DEFAULT_IMAGE_PATCH_TOKEN]] = -100\n",
    "    inputs[\"labels\"][inputs[\"labels\"] == processor.special_token_ids[DEFAULT_IM_START_TOKEN]] = -100\n",
    "    inputs[\"labels\"][inputs[\"labels\"] == processor.special_token_ids[DEFAULT_IM_END_TOKEN]] = -100\n",
    "    inputs[\"labels\"][inputs[\"labels\"] == processor.special_token_ids[DEFAULT_IM_COL_TOKEN]] = -100\n",
    "    \n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<im_start>': 152064,\n",
       " '<im_end>': 152065,\n",
       " '<im_patch>': 152066,\n",
       " '<im_col>': 152067,\n",
       " '<|image|>': 152068}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.special_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    '''\n",
    "    examples:\n",
    "    \n",
    "        input_ids torch.Size([B, 1249])\n",
    "        images torch.Size([B, 13, 576, 588])\n",
    "        image_input_idx torch.Size([B, 13, 144])\n",
    "        image_masks torch.Size([B, 13, 576])\n",
    "        labels torch.Size([B, 1249])\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # examples has keys: input_ids', 'images', 'image_input_idx', 'image_masks', 'labels'\n",
    "    \n",
    "    padded_inputs = tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": [example[\"input_ids\"][0] for example in examples],\n",
    "        },\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    labels = padded_inputs[\"input_ids\"].clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # TODO\n",
    "    labels[labels == processor.special_token_ids[IMAGE_PROMPT]] = -100\n",
    "    labels[labels == processor.special_token_ids[DEFAULT_IMAGE_PATCH_TOKEN]] = -100\n",
    "    labels[labels == processor.special_token_ids[DEFAULT_IM_START_TOKEN]] = -100\n",
    "    labels[labels == processor.special_token_ids[DEFAULT_IM_END_TOKEN]] = -100\n",
    "    labels[labels == processor.special_token_ids[DEFAULT_IM_COL_TOKEN]] = -100\n",
    "    \n",
    "    padded_inputs[\"labels\"] = labels\n",
    "    \n",
    "    padded_inputs[\"images\"] = torch.cat([example[\"images\"] for example in examples], 0)\n",
    "    padded_inputs[\"image_input_idx\"] = torch.cat([example[\"image_input_idx\"] for example in examples], 0)\n",
    "    padded_inputs[\"image_masks\"] = torch.cat([example[\"image_masks\"] for example in examples], 0)\n",
    "    \n",
    "    padded_inputs = padded_inputs.to(torch.bfloat16).to(\"cuda\")\n",
    "    \n",
    "    return padded_inputs\n",
    "    \n",
    "    # padded_inputs = tokenizer.pad(\n",
    "    #     {\n",
    "    #         \"input_ids\": [example[\"input_ids\"] for example in examples],\n",
    "    #     },\n",
    "    #     padding=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "    \n",
    "    # labels = padded_inputs[\"input_ids\"].clone()\n",
    "    # labels[labels == tokenizer.pad_token_id] = -100\n",
    "    # padded_inputs[\"labels\"] = labels\n",
    "    \n",
    "    # # padded_inputs[\"images\"] = torch.cat([example[\"images\"] for example in examples], 0)\n",
    "    # padded_inputs[\"images\"] = torch.cat([torch.tensor(example[\"images\"]) for example in examples], dim=0)\n",
    "    # padded_inputs[\"image_input_idx\"] = torch.cat([example[\"image_input_idx\"] for example in examples], 0)\n",
    "    # padded_inputs[\"image_masks\"] = torch.cat([example[\"image_masks\"] for example in examples], 0)\n",
    "    \n",
    "    # return padded_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90538785d0184930a0ac0451d31e98cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map dataset with process function\n",
    "dataset_mapped = dataset.map(process, batched=False, batch_size=4, remove_columns=dataset.column_names)\n",
    "\n",
    "dataset_mapped = dataset_mapped.with_format(\"torch\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process(dataset[0])['input_ids'].to(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151643, 152064, 152066,  ...,   5053,  21388,     25]],\n",
       "        device='cuda:0'),\n",
       " 'images': tensor([[[[-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           ...,\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802]],\n",
       " \n",
       "          [[-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           ...,\n",
       "           [ 1.8865, -0.2513, -0.7550,  ...,  1.9019, -0.1943, -0.7327],\n",
       "           [ 1.8772, -0.2609, -0.7641,  ...,  1.9119, -0.2261, -0.7241],\n",
       "           [ 1.8865, -0.2513, -0.7550,  ...,  1.9303, -0.3710, -0.7925]],\n",
       " \n",
       "          [[-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           ...,\n",
       "           [ 1.5685,  0.7125,  0.2205,  ...,  1.7768,  0.5018, -0.7248],\n",
       "           [ 1.2355,  0.7832,  0.2811,  ...,  1.9097,  1.8764,  1.2966],\n",
       "           [ 1.7787,  1.1752, -0.2208,  ...,  1.5817,  1.5702,  0.5110]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.7118,  1.6246,  1.5360,  ...,  1.7228,  1.7696,  1.6935],\n",
       "           [ 1.8428,  1.9039,  1.8593,  ..., -0.5700,  1.8288, -0.7217],\n",
       "           [-1.5806,  1.1428, -1.3166,  ..., -1.1296,  0.9381, -1.2166],\n",
       "           ...,\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802]],\n",
       " \n",
       "          [[-1.1631, -1.0202, -0.4392,  ..., -1.0252, -1.6447, -1.3198],\n",
       "           [ 1.8991, -0.4917, -0.9628,  ..., -0.5918, -0.6384,  1.5371],\n",
       "           [-0.6006, -0.5227,  1.3889,  ..., -0.6191, -0.4933,  1.4878],\n",
       "           ...,\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802]],\n",
       " \n",
       "          [[-0.8974, -1.3054, -1.1742,  ...,  1.9167, -0.5238, -0.8631],\n",
       "           [ 1.7887, -0.0766, -0.6176,  ...,  1.8922, -0.4884, -0.9267],\n",
       "           [-1.3575, -1.3763, -0.8719,  ..., -0.6392, -1.2547, -1.1230],\n",
       "           ...,\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802],\n",
       "           [-1.7923, -1.7521, -1.4802,  ..., -1.7923, -1.7521, -1.4802]]]],\n",
       "        device='cuda:0'),\n",
       " 'image_input_idx': tensor([[[   2,    3,    4,  ...,  154,  155,  156],\n",
       "          [ 160,  161,  162,  ..., -100, -100, -100],\n",
       "          [-100, -100,  170,  ..., -100, -100, -100],\n",
       "          ...,\n",
       "          [-100, -100, -100,  ..., 1176, -100, -100],\n",
       "          [-100, -100, -100,  ..., 1184, -100, -100],\n",
       "          [-100, -100, -100,  ..., 1192, 1193, 1194]]], device='cuda:0'),\n",
       " 'image_masks': tensor([[[ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "          ...,\n",
       "          [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]], device='cuda:0'),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  ...,  5053, 21388,    25]], device='cuda:0')}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mapped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset_mapped[0]['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall minimum label value: -100\n",
      "Overall maximum label value: 21388\n",
      "Vocabulary size: 151643\n",
      "Minimum valid label value: 11\n",
      "Maximum valid label value: 21388\n",
      "All label values are within the valid range [0, 151642].\n"
     ]
    }
   ],
   "source": [
    "# Check overall min and max in the labels tensor\n",
    "min_label = labels.min().item()\n",
    "max_label = labels.max().item()\n",
    "print(\"Overall minimum label value:\", min_label)\n",
    "print(\"Overall maximum label value:\", max_label)\n",
    "\n",
    "# Define your vocabulary size (adjust this value as needed)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Filter out the ignore_index values (commonly -100)\n",
    "valid_labels = labels[labels != -100]\n",
    "\n",
    "if valid_labels.numel() > 0:\n",
    "    valid_min = valid_labels.min().item()\n",
    "    valid_max = valid_labels.max().item()\n",
    "    print(\"Minimum valid label value:\", valid_min)\n",
    "    print(\"Maximum valid label value:\", valid_max)\n",
    "else:\n",
    "    print(\"No valid labels found (all labels might be set to ignore index).\")\n",
    "\n",
    "# Check if any valid label is out-of-range\n",
    "if ((valid_labels < 0) | (valid_labels >= vocab_size)).any():\n",
    "    print(\"There are label values out of the valid range [0, {}].\".format(vocab_size - 1))\n",
    "else:\n",
    "    print(\"All label values are within the valid range [0, {}].\".format(vocab_size - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args=TrainingArguments(\n",
    "            num_train_epochs=10,\n",
    "            remove_unused_columns=False,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=1,\n",
    "            warmup_steps=1,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=1e-6,\n",
    "            adam_beta2=0.999,\n",
    "\n",
    "            save_strategy=\"no\", #TODO: change to 'steps' or 'epoch'\n",
    "            # optim=\"adamw_hf\",\n",
    "            optim=\"adamw_torch\",\n",
    "            push_to_hub=True,\n",
    "            save_total_limit=1,\n",
    "            bf16=True,\n",
    "            output_dir=\"./lora-molmo-pixmo\",\n",
    "            \n",
    "            logging_strategy=\"steps\",\n",
    "            report_to=\"wandb\",\n",
    "            logging_steps=10,\n",
    "            \n",
    "            dataloader_pin_memory=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-24 20:58:38,601] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_mapped,\n",
    "        data_collator=collate_fn,\n",
    "        args=training_args\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshehanmunasinghe\u001b[0m (\u001b[33mshehanmunasinghe-mbzuai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699c38d249b84ca58c46bee4a31df2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111295639226834, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/share/users/shehan/workspace_pointing_lmm/MolmoVideo/wandb/run-20250224_205852-r0d5leli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shehanmunasinghe-mbzuai/huggingface/runs/r0d5leli' target=\"_blank\">./lora-molmo-pixmo</a></strong> to <a href='https://wandb.ai/shehanmunasinghe-mbzuai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shehanmunasinghe-mbzuai/huggingface' target=\"_blank\">https://wandb.ai/shehanmunasinghe-mbzuai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shehanmunasinghe-mbzuai/huggingface/runs/r0d5leli' target=\"_blank\">https://wandb.ai/shehanmunasinghe-mbzuai/huggingface/runs/r0d5leli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n",
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> images.shape torch.Size([4, 13, 576, 588])\n",
      ">> image_features.shape torch.Size([4, 13, 576, 2048])\n",
      ">> cls_embed.shape torch.Size([4, 13, 2048])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 754.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 697.88 MiB is free. Including non-PyTorch memory, this process has 38.79 GiB memory in use. Of the allocated memory 38.01 GiB is allocated by PyTorch, and 292.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/transformers/trainer.py:2155\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2162\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/transformers/trainer.py:2522\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2516\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2517\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2520\u001b[0m )\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2528\u001b[0m ):\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/transformers/trainer.py:3655\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3653\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3655\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3661\u001b[0m ):\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/transformers/trainer.py:3709\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3707\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3708\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3709\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3710\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3711\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/users/shehan/workspace_pointing_lmm/MolmoVideo/molmo/model.py:2125\u001b[0m, in \u001b[0;36mMolmoForCausalLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, attention_bias, response_mask, images, image_masks, image_input_idx, subsegment_ids, position_ids, past_key_values, labels, loss_masks, use_cache, last_logits_only, output_attentions, output_hidden_states, append_last_valid_logits, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   2122\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 2125\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_input_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_input_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubsegment_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubsegment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_logits_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_logits_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend_last_valid_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend_last_valid_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m   2144\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n",
      "File \u001b[0;32m/share/users/shehan/workspace_pointing_lmm/MolmoVideo/molmo/model.py:1973\u001b[0m, in \u001b[0;36mMolmo.forward\u001b[0;34m(self, input_ids, input_embeddings, attention_mask, attention_bias, response_mask, images, image_masks, image_input_idx, subsegment_ids, position_ids, past_key_values, use_cache, last_logits_only, output_hidden_states, append_last_valid_logits)\u001b[0m\n\u001b[1;32m   1970\u001b[0m     all_hidden_states\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m   1972\u001b[0m layer_past \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m past_key_values[block_idx]\n\u001b[0;32m-> 1973\u001b[0m x, cache \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1976\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/share/users/shehan/workspace_pointing_lmm/MolmoVideo/molmo/model.py:512\u001b[0m, in \u001b[0;36mMolmoSequentialBlock.forward\u001b[0;34m(self, x, attention_bias, position_ids, layer_past, use_cache)\u001b[0m\n\u001b[1;32m    508\u001b[0m     att, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention, q, k, v, attention_bias, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, layer_past\u001b[38;5;241m=\u001b[39mlayer_past, use_cache\u001b[38;5;241m=\u001b[39muse_cache\n\u001b[1;32m    510\u001b[0m     )\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     att, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnorm_after:\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/share/users/shehan/workspace_pointing_lmm/MolmoVideo/molmo/model.py:404\u001b[0m, in \u001b[0;36mMolmoBlock.attention\u001b[0;34m(self, q, k, v, attention_bias, position_ids, layer_past, use_cache)\u001b[0m\n\u001b[1;32m    398\u001b[0m     attention_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_attn_bias(\n\u001b[1;32m    399\u001b[0m         attention_bias[:, :, key_len \u001b[38;5;241m-\u001b[39m query_len : key_len, :key_len], dtype\n\u001b[1;32m    400\u001b[0m     )\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Get the attention scores.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# shape: (B, nh, T, hs)\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_dropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_attention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Re-assemble all head outputs side-by-side.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m att \u001b[38;5;241m=\u001b[39m att\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, C)\n",
      "File \u001b[0;32m/share/users/shehan/workspace_pointing_lmm/MolmoVideo/molmo/model.py:341\u001b[0m, in \u001b[0;36mMolmoBlock._scaled_dot_product_attention\u001b[0;34m(self, q, k, v, attn_mask, dropout_p, response_dropout_p, is_causal)\u001b[0m\n\u001b[1;32m    338\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mrepeat_interleave(num_q_heads \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_kv_heads, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_size\u001b[38;5;241m=\u001b[39mnum_q_heads)\n\u001b[1;32m    339\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mrepeat_interleave(num_q_heads \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_kv_heads, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_size\u001b[38;5;241m=\u001b[39mnum_q_heads)\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 754.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 697.88 MiB is free. Including non-PyTorch memory, this process has 38.79 GiB memory in use. Of the allocated memory 38.01 GiB is allocated by PyTorch, and 292.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
