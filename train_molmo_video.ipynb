{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from molmo_video.preprocessor import MolmoProcessor\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/Molmo-7B-D-0924\")\n",
    "\n",
    "\n",
    "processor = MolmoProcessor(tokenizer=tokenizer, \n",
    "                           preprocessor_config_path=\"/share/users/shehan/workspace_pointing_lmm/MolmoVideo/molmo_video/hf_configs_Molmo-7B-D-0924/preprocessor_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molmo_video.preprocessor import (DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, DEFAULT_IM_COL_TOKEN, IMAGE_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are given {num_frames} frames from a video. The frame indices are {selected_frame_idxs}. \"\n",
    "PROMPT_TEMPLATES = [\n",
    "        \"Point to {label}\\nPlease say 'This isn't in the video.' if it is not in the video.\",\n",
    "        \"Point to all occurrences of \\\"{label}\\\"\",\n",
    "        \"Point to any {label} in the video\",\n",
    "        \"Point to any {label} in the video.\",\n",
    "        \"Point: Where are the {label}\",\n",
    "        \"Show me where the {label} are\",\n",
    "        \"Can you show me where the {label} are?\",\n",
    "        \"Show me where the {label} are\",\n",
    "        \"Show me where a {label} is\",\n",
    "        \"Show me where a {label} is.\",\n",
    "        \"If there are any {label} in the video? Show me where they are.\",\n",
    "        \"Where are the {label}?\",\n",
    "        \"Generate a list of points showing where the {label} are.\",\n",
    "        \"Find the \\\"{label}\\\".\",\n",
    "        \"Find a \\\"{label}\\\".\",\n",
    "        \"Locate all {label}.\",\n",
    "        \"Locate an {label}.\",\n",
    "        \"Locate a {label}.\",\n",
    "        \"Locate every {label}.\",\n",
    "        \"Locate {label}.\",\n",
    "        \"Locate the {label}.\",\n",
    "        \"Object: {label}\\nInstruction: Point to the object.\",\n",
    "        \"find {label}\",\n",
    "        \"find {label}.\",\n",
    "        \"Point to every {label}\",\n",
    "        \"find any {label} in the picture\",\n",
    "        \"Find the {label}\",\n",
    "        \"Find any {label}\",\n",
    "        \"Point to a {label}\",\n",
    "        \"Point to an {label}\",\n",
    "        \"Look for {label} in the video and show me where they are.\",\n",
    "        \"Help me find an object in the video by pointing to them.\\nObject: {label}.\",\n",
    "        \"I am looking for {label}, where can they be found in the video?\",\n",
    "        \"Can you see any {label} in the video? Point to them.\",\n",
    "        \"Point out each {label} in the video.\",\n",
    "        \"Point out every {label} in the video.\",\n",
    "        \"Point to the {label} in the video.\",\n",
    "        \"Locate each {label} in the video.\",\n",
    "        \"Can you point out all {label} in this video?\",\n",
    "        \"Please find {label} and show me where they are.\",\n",
    "        \"If there are any {label} present, indicate their positions.\",\n",
    "        \"If there is a {label} present, indicate its positions.\",\n",
    "        \"show me all visible {label}\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video': 'davis17/00000/frames', 'frame_idxs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81], 'timestamps': [0.0, 0.03333333333333333, 0.06666666666666667, 0.1, 0.13333333333333333, 0.16666666666666666, 0.2, 0.23333333333333334, 0.26666666666666666, 0.3, 0.3333333333333333, 0.36666666666666664, 0.4, 0.43333333333333335, 0.4666666666666667, 0.5, 0.5333333333333333, 0.5666666666666667, 0.6, 0.6333333333333333, 0.6666666666666666, 0.7, 0.7333333333333333, 0.7666666666666667, 0.8, 0.8333333333333334, 0.8666666666666667, 0.9, 0.9333333333333333, 0.9666666666666667, 1.0, 1.0333333333333334, 1.0666666666666667, 1.1, 1.1333333333333333, 1.1666666666666667, 1.2, 1.2333333333333334, 1.2666666666666666, 1.3, 1.3333333333333333, 1.3666666666666667, 1.4, 1.4333333333333333, 1.4666666666666666, 1.5, 1.5333333333333334, 1.5666666666666667, 1.6, 1.6333333333333333, 1.6666666666666667, 1.7, 1.7333333333333334, 1.7666666666666666, 1.8, 1.8333333333333333, 1.8666666666666667, 1.9, 1.9333333333333333, 1.9666666666666666, 2.0, 2.033333333333333, 2.066666666666667, 2.1, 2.1333333333333333, 2.1666666666666665, 2.2, 2.2333333333333334, 2.2666666666666666, 2.3, 2.3333333333333335, 2.3666666666666667, 2.4, 2.433333333333333, 2.466666666666667, 2.5, 2.533333333333333, 2.566666666666667, 2.6, 2.6333333333333333, 2.6666666666666665, 2.7], 'points': {'0': [53.0, 183.30864197530863], '1': [52.5, 183.30864197530863], '2': [52.25, 182.51851851851853], '3': [52.0, 181.72839506172838], '4': [51.75, 181.72839506172838], '5': [51.5, 181.72839506172838], '6': [51.5, 180.93827160493825], '7': [51.5, 180.93827160493825], '8': [51.75, 180.93827160493825], '9': [52.0, 180.93827160493825], '10': [52.25, 181.72839506172838], '11': [52.5, 181.72839506172838], '12': [52.75, 182.51851851851853], '13': [53.0, 182.51851851851853], '14': [53.25, 183.30864197530863], '15': [53.5, 184.0987654320988], '16': [53.75, 184.0987654320988], '17': [54.25, 184.0987654320988], '18': [54.5, 184.88888888888889], '19': [55.0, 184.88888888888889], '20': [55.5, 185.67901234567904], '21': [55.5, 184.0987654320988], '22': [55.75, 184.88888888888889], '23': [56.0, 185.67901234567904], '24': [56.0, 186.46913580246914], '25': [56.0, 187.25925925925924], '26': [55.75, 188.0493827160494], '27': [55.75, 188.8395061728395], '28': [55.75, 189.6296296296296], '29': [55.75, 191.2098765432099], '30': [55.75, 192.0], '31': [55.75, 192.0], '32': [56.0, 192.79012345679016], '33': [56.0, 192.79012345679016], '34': [56.0, 192.79012345679016], '35': [56.0, 193.58024691358025], '36': [56.0, 193.58024691358025], '37': [55.75, 194.37037037037035], '38': [55.75, 194.37037037037035], '39': [55.5, 194.37037037037035], '40': [55.5, 194.37037037037035], '41': [55.5, 194.37037037037035], '42': [55.5, 195.1604938271605], '43': [55.5, 195.1604938271605], '44': [55.5, 195.1604938271605], '45': [55.75, 195.1604938271605], '46': [55.75, 195.1604938271605], '47': [55.75, 195.1604938271605], '48': [56.0, 194.37037037037035], '49': [56.0, 194.37037037037035], '50': [56.0, 192.79012345679016], '51': [56.0, 191.2098765432099], '52': [55.75, 188.8395061728395], '53': [55.5, 185.67901234567904], '54': [55.25, 183.30864197530863], '55': [55.0, 180.14814814814812], '56': [54.75, 177.77777777777777], '57': [54.5, 175.40740740740742], '58': [54.25, 173.82716049382717], '59': [54.0, 171.45679012345678], '60': [53.75, 169.87654320987653], '61': [53.5, 168.29629629629628], '62': [53.5, 167.50617283950618], '63': [53.25, 165.92592592592592], '64': [53.25, 164.34567901234567], '65': [53.25, 163.55555555555554], '66': [53.25, 161.97530864197532], '67': [53.25, 161.1851851851852], '68': [53.0, 160.39506172839506], '69': [53.0, 159.60493827160494], '70': [52.75, 158.8148148148148], '71': [52.5, 158.02469135802468], '72': [52.25, 157.23456790123458], '73': [52.25, 156.44444444444446], '74': [52.0, 156.44444444444446], '75': [51.75, 156.44444444444446], '76': [51.5, 156.44444444444446], '77': [50.99999999999999, 155.65432098765433], '78': [50.75, 155.65432098765433], '79': [50.5, 155.65432098765433], '80': [50.0, 155.65432098765433], '81': [50.0, 154.8641975308642], '85': None, '86': None, '88': None, '89': None, '82': None, '83': None, '84': None, '87': None, '90': None, '91': None, '92': None, '93': None, '94': None, '95': None, '96': None, '97': None, '98': None, '99': None}, 'caption': 'a brown bear', 'fps': 30}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the JSONL file\n",
    "dataset = load_dataset(\"json\", \n",
    "                       data_files=\"/share/users/shehan/workspace_pointing_lmm/DataPrep/annotations_davis17_train.jsonl\",\n",
    "                       split=\"train\")\n",
    "\n",
    "# Print a sample to verify\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['video', 'frame_idxs', 'timestamps', 'points', 'caption', 'fps'],\n",
       "    num_rows: 572\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# base_data_dir = '/share/users/shehan/workspace_pointing_lmm/DataPrep'\n",
    "\n",
    "# def load_images(example):\n",
    "#     # List to hold the loaded images\n",
    "#     images = []\n",
    "#     # Loop over each frame index in the example\n",
    "#     for idx in example['frame_idxs']:\n",
    "#         # Construct the full file path for the image.\n",
    "#         # 'video' contains the directory, e.g. 'davis17/00000/frames'\n",
    "#         # The image file is assumed to be formatted as \"00000.jpg\", \"00001.jpg\", etc.\n",
    "#         frame_filename = f\"{idx:05d}.jpg\"\n",
    "#         frame_path = os.path.join(base_data_dir, example['video'], frame_filename)\n",
    "        \n",
    "#         # Open and convert the image to RGB (or keep as-is if desired)\n",
    "#         try:\n",
    "#             image = Image.open(frame_path).convert(\"RGB\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading image {frame_path}: {e}\")\n",
    "#             image = None\n",
    "#         images.append(image)\n",
    "    \n",
    "#     # Optionally, add the list of images to the example under a new key\n",
    "#     example[\"images\"] = images\n",
    "#     return example\n",
    "\n",
    "# # Use the map function to apply the load_images function to every example in the dataset\n",
    "# dataset_with_images = dataset.map(load_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': 'davis17/00000/frames',\n",
       " 'frame_idxs': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81],\n",
       " 'timestamps': [0.0,\n",
       "  0.03333333333333333,\n",
       "  0.06666666666666667,\n",
       "  0.1,\n",
       "  0.13333333333333333,\n",
       "  0.16666666666666666,\n",
       "  0.2,\n",
       "  0.23333333333333334,\n",
       "  0.26666666666666666,\n",
       "  0.3,\n",
       "  0.3333333333333333,\n",
       "  0.36666666666666664,\n",
       "  0.4,\n",
       "  0.43333333333333335,\n",
       "  0.4666666666666667,\n",
       "  0.5,\n",
       "  0.5333333333333333,\n",
       "  0.5666666666666667,\n",
       "  0.6,\n",
       "  0.6333333333333333,\n",
       "  0.6666666666666666,\n",
       "  0.7,\n",
       "  0.7333333333333333,\n",
       "  0.7666666666666667,\n",
       "  0.8,\n",
       "  0.8333333333333334,\n",
       "  0.8666666666666667,\n",
       "  0.9,\n",
       "  0.9333333333333333,\n",
       "  0.9666666666666667,\n",
       "  1.0,\n",
       "  1.0333333333333334,\n",
       "  1.0666666666666667,\n",
       "  1.1,\n",
       "  1.1333333333333333,\n",
       "  1.1666666666666667,\n",
       "  1.2,\n",
       "  1.2333333333333334,\n",
       "  1.2666666666666666,\n",
       "  1.3,\n",
       "  1.3333333333333333,\n",
       "  1.3666666666666667,\n",
       "  1.4,\n",
       "  1.4333333333333333,\n",
       "  1.4666666666666666,\n",
       "  1.5,\n",
       "  1.5333333333333334,\n",
       "  1.5666666666666667,\n",
       "  1.6,\n",
       "  1.6333333333333333,\n",
       "  1.6666666666666667,\n",
       "  1.7,\n",
       "  1.7333333333333334,\n",
       "  1.7666666666666666,\n",
       "  1.8,\n",
       "  1.8333333333333333,\n",
       "  1.8666666666666667,\n",
       "  1.9,\n",
       "  1.9333333333333333,\n",
       "  1.9666666666666666,\n",
       "  2.0,\n",
       "  2.033333333333333,\n",
       "  2.066666666666667,\n",
       "  2.1,\n",
       "  2.1333333333333333,\n",
       "  2.1666666666666665,\n",
       "  2.2,\n",
       "  2.2333333333333334,\n",
       "  2.2666666666666666,\n",
       "  2.3,\n",
       "  2.3333333333333335,\n",
       "  2.3666666666666667,\n",
       "  2.4,\n",
       "  2.433333333333333,\n",
       "  2.466666666666667,\n",
       "  2.5,\n",
       "  2.533333333333333,\n",
       "  2.566666666666667,\n",
       "  2.6,\n",
       "  2.6333333333333333,\n",
       "  2.6666666666666665,\n",
       "  2.7],\n",
       " 'points': {'0': [53.0, 183.30864197530863],\n",
       "  '1': [52.5, 183.30864197530863],\n",
       "  '2': [52.25, 182.51851851851853],\n",
       "  '3': [52.0, 181.72839506172838],\n",
       "  '4': [51.75, 181.72839506172838],\n",
       "  '5': [51.5, 181.72839506172838],\n",
       "  '6': [51.5, 180.93827160493825],\n",
       "  '7': [51.5, 180.93827160493825],\n",
       "  '8': [51.75, 180.93827160493825],\n",
       "  '9': [52.0, 180.93827160493825],\n",
       "  '10': [52.25, 181.72839506172838],\n",
       "  '11': [52.5, 181.72839506172838],\n",
       "  '12': [52.75, 182.51851851851853],\n",
       "  '13': [53.0, 182.51851851851853],\n",
       "  '14': [53.25, 183.30864197530863],\n",
       "  '15': [53.5, 184.0987654320988],\n",
       "  '16': [53.75, 184.0987654320988],\n",
       "  '17': [54.25, 184.0987654320988],\n",
       "  '18': [54.5, 184.88888888888889],\n",
       "  '19': [55.0, 184.88888888888889],\n",
       "  '20': [55.5, 185.67901234567904],\n",
       "  '21': [55.5, 184.0987654320988],\n",
       "  '22': [55.75, 184.88888888888889],\n",
       "  '23': [56.0, 185.67901234567904],\n",
       "  '24': [56.0, 186.46913580246914],\n",
       "  '25': [56.0, 187.25925925925924],\n",
       "  '26': [55.75, 188.0493827160494],\n",
       "  '27': [55.75, 188.8395061728395],\n",
       "  '28': [55.75, 189.6296296296296],\n",
       "  '29': [55.75, 191.2098765432099],\n",
       "  '30': [55.75, 192.0],\n",
       "  '31': [55.75, 192.0],\n",
       "  '32': [56.0, 192.79012345679016],\n",
       "  '33': [56.0, 192.79012345679016],\n",
       "  '34': [56.0, 192.79012345679016],\n",
       "  '35': [56.0, 193.58024691358025],\n",
       "  '36': [56.0, 193.58024691358025],\n",
       "  '37': [55.75, 194.37037037037035],\n",
       "  '38': [55.75, 194.37037037037035],\n",
       "  '39': [55.5, 194.37037037037035],\n",
       "  '40': [55.5, 194.37037037037035],\n",
       "  '41': [55.5, 194.37037037037035],\n",
       "  '42': [55.5, 195.1604938271605],\n",
       "  '43': [55.5, 195.1604938271605],\n",
       "  '44': [55.5, 195.1604938271605],\n",
       "  '45': [55.75, 195.1604938271605],\n",
       "  '46': [55.75, 195.1604938271605],\n",
       "  '47': [55.75, 195.1604938271605],\n",
       "  '48': [56.0, 194.37037037037035],\n",
       "  '49': [56.0, 194.37037037037035],\n",
       "  '50': [56.0, 192.79012345679016],\n",
       "  '51': [56.0, 191.2098765432099],\n",
       "  '52': [55.75, 188.8395061728395],\n",
       "  '53': [55.5, 185.67901234567904],\n",
       "  '54': [55.25, 183.30864197530863],\n",
       "  '55': [55.0, 180.14814814814812],\n",
       "  '56': [54.75, 177.77777777777777],\n",
       "  '57': [54.5, 175.40740740740742],\n",
       "  '58': [54.25, 173.82716049382717],\n",
       "  '59': [54.0, 171.45679012345678],\n",
       "  '60': [53.75, 169.87654320987653],\n",
       "  '61': [53.5, 168.29629629629628],\n",
       "  '62': [53.5, 167.50617283950618],\n",
       "  '63': [53.25, 165.92592592592592],\n",
       "  '64': [53.25, 164.34567901234567],\n",
       "  '65': [53.25, 163.55555555555554],\n",
       "  '66': [53.25, 161.97530864197532],\n",
       "  '67': [53.25, 161.1851851851852],\n",
       "  '68': [53.0, 160.39506172839506],\n",
       "  '69': [53.0, 159.60493827160494],\n",
       "  '70': [52.75, 158.8148148148148],\n",
       "  '71': [52.5, 158.02469135802468],\n",
       "  '72': [52.25, 157.23456790123458],\n",
       "  '73': [52.25, 156.44444444444446],\n",
       "  '74': [52.0, 156.44444444444446],\n",
       "  '75': [51.75, 156.44444444444446],\n",
       "  '76': [51.5, 156.44444444444446],\n",
       "  '77': [50.99999999999999, 155.65432098765433],\n",
       "  '78': [50.75, 155.65432098765433],\n",
       "  '79': [50.5, 155.65432098765433],\n",
       "  '80': [50.0, 155.65432098765433],\n",
       "  '81': [50.0, 154.8641975308642],\n",
       "  '85': None,\n",
       "  '86': None,\n",
       "  '88': None,\n",
       "  '89': None,\n",
       "  '82': None,\n",
       "  '83': None,\n",
       "  '84': None,\n",
       "  '87': None,\n",
       "  '90': None,\n",
       "  '91': None,\n",
       "  '92': None,\n",
       "  '93': None,\n",
       "  '94': None,\n",
       "  '95': None,\n",
       "  '96': None,\n",
       "  '97': None,\n",
       "  '98': None,\n",
       "  '99': None},\n",
       " 'caption': 'a brown bear',\n",
       " 'fps': 30}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [53.0, 183.30864197530863],\n",
       " '1': [52.5, 183.30864197530863],\n",
       " '2': [52.25, 182.51851851851853],\n",
       " '3': [52.0, 181.72839506172838],\n",
       " '4': [51.75, 181.72839506172838],\n",
       " '5': [51.5, 181.72839506172838],\n",
       " '6': [51.5, 180.93827160493825],\n",
       " '7': [51.5, 180.93827160493825],\n",
       " '8': [51.75, 180.93827160493825],\n",
       " '9': [52.0, 180.93827160493825],\n",
       " '10': [52.25, 181.72839506172838],\n",
       " '11': [52.5, 181.72839506172838],\n",
       " '12': [52.75, 182.51851851851853],\n",
       " '13': [53.0, 182.51851851851853],\n",
       " '14': [53.25, 183.30864197530863],\n",
       " '15': [53.5, 184.0987654320988],\n",
       " '16': [53.75, 184.0987654320988],\n",
       " '17': [54.25, 184.0987654320988],\n",
       " '18': [54.5, 184.88888888888889],\n",
       " '19': [55.0, 184.88888888888889],\n",
       " '20': [55.5, 185.67901234567904],\n",
       " '21': [55.5, 184.0987654320988],\n",
       " '22': [55.75, 184.88888888888889],\n",
       " '23': [56.0, 185.67901234567904],\n",
       " '24': [56.0, 186.46913580246914],\n",
       " '25': [56.0, 187.25925925925924],\n",
       " '26': [55.75, 188.0493827160494],\n",
       " '27': [55.75, 188.8395061728395],\n",
       " '28': [55.75, 189.6296296296296],\n",
       " '29': [55.75, 191.2098765432099],\n",
       " '30': [55.75, 192.0],\n",
       " '31': [55.75, 192.0],\n",
       " '32': [56.0, 192.79012345679016],\n",
       " '33': [56.0, 192.79012345679016],\n",
       " '34': [56.0, 192.79012345679016],\n",
       " '35': [56.0, 193.58024691358025],\n",
       " '36': [56.0, 193.58024691358025],\n",
       " '37': [55.75, 194.37037037037035],\n",
       " '38': [55.75, 194.37037037037035],\n",
       " '39': [55.5, 194.37037037037035],\n",
       " '40': [55.5, 194.37037037037035],\n",
       " '41': [55.5, 194.37037037037035],\n",
       " '42': [55.5, 195.1604938271605],\n",
       " '43': [55.5, 195.1604938271605],\n",
       " '44': [55.5, 195.1604938271605],\n",
       " '45': [55.75, 195.1604938271605],\n",
       " '46': [55.75, 195.1604938271605],\n",
       " '47': [55.75, 195.1604938271605],\n",
       " '48': [56.0, 194.37037037037035],\n",
       " '49': [56.0, 194.37037037037035],\n",
       " '50': [56.0, 192.79012345679016],\n",
       " '51': [56.0, 191.2098765432099],\n",
       " '52': [55.75, 188.8395061728395],\n",
       " '53': [55.5, 185.67901234567904],\n",
       " '54': [55.25, 183.30864197530863],\n",
       " '55': [55.0, 180.14814814814812],\n",
       " '56': [54.75, 177.77777777777777],\n",
       " '57': [54.5, 175.40740740740742],\n",
       " '58': [54.25, 173.82716049382717],\n",
       " '59': [54.0, 171.45679012345678],\n",
       " '60': [53.75, 169.87654320987653],\n",
       " '61': [53.5, 168.29629629629628],\n",
       " '62': [53.5, 167.50617283950618],\n",
       " '63': [53.25, 165.92592592592592],\n",
       " '64': [53.25, 164.34567901234567],\n",
       " '65': [53.25, 163.55555555555554],\n",
       " '66': [53.25, 161.97530864197532],\n",
       " '67': [53.25, 161.1851851851852],\n",
       " '68': [53.0, 160.39506172839506],\n",
       " '69': [53.0, 159.60493827160494],\n",
       " '70': [52.75, 158.8148148148148],\n",
       " '71': [52.5, 158.02469135802468],\n",
       " '72': [52.25, 157.23456790123458],\n",
       " '73': [52.25, 156.44444444444446],\n",
       " '74': [52.0, 156.44444444444446],\n",
       " '75': [51.75, 156.44444444444446],\n",
       " '76': [51.5, 156.44444444444446],\n",
       " '77': [50.99999999999999, 155.65432098765433],\n",
       " '78': [50.75, 155.65432098765433],\n",
       " '79': [50.5, 155.65432098765433],\n",
       " '80': [50.0, 155.65432098765433],\n",
       " '81': [50.0, 154.8641975308642],\n",
       " '85': None,\n",
       " '86': None,\n",
       " '88': None,\n",
       " '89': None,\n",
       " '82': None,\n",
       " '83': None,\n",
       " '84': None,\n",
       " '87': None,\n",
       " '90': None,\n",
       " '91': None,\n",
       " '92': None,\n",
       " '93': None,\n",
       " '94': None,\n",
       " '95': None,\n",
       " '96': None,\n",
       " '97': None,\n",
       " '98': None,\n",
       " '99': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points_in_xml_format(points, caption):\n",
    "    \"\"\"\n",
    "    Convert a list of (t, x, y) points to a string in XML format.\n",
    "    \"\"\"\n",
    "    lines = [\"<point\"]\n",
    "    # for t, x, y in points:\n",
    "    for t, x_y in points.items():\n",
    "        if x_y is None:\n",
    "            continue\n",
    "        x, y = x_y\n",
    "        # Append each point's attributes on a new, indented line\n",
    "        lines.append(f' t=\"{int(t)}\" x=\"{x:.1f}\" y=\"{y:.1f}\"')\n",
    "    # Append the alt attribute and the element's text content on the final line\n",
    "    lines.append(f' alt=\"{caption}\">{caption}</point>')\n",
    "    formatted_output = \"\".join(lines)\n",
    "    \n",
    "    return formatted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "base_data_dir = '/share/users/shehan/workspace_pointing_lmm/DataPrep'\n",
    "\n",
    "num_frames = 4\n",
    "\n",
    "def random_augmentation(batch):\n",
    "    \"\"\"\n",
    "    Batch-wise random augmentation.\n",
    "    For each example in the batch:\n",
    "      - Randomly selects `num_frames` indices (or all if there are fewer).\n",
    "      - Loads only the images corresponding to the selected frame indices.\n",
    "      - Subsets the 'frame_idxs', 'timestamps', and 'points' to only those indices.\n",
    "    Assumes that each example in the batch has keys:\n",
    "      'video'      : the directory containing the frames,\n",
    "      'frame_idxs' : a list of frame indices,\n",
    "      'timestamps' : a list of timestamps,\n",
    "      'points'     : a list of (t, x, y) values.\n",
    "    \"\"\"\n",
    "    new_batch = {\"images\": [], \"frame_idxs\": [], \"timestamps\": [], \"points\": [], \"question\": [], \"answer\": []}\n",
    "\n",
    "    # Process each example in the batch\n",
    "    for i in range(len(batch[\"video\"])):\n",
    "        video_dir = batch[\"video\"][i]\n",
    "        frame_idxs = batch[\"frame_idxs\"][i]    # e.g. [0, 1, 2, ...]\n",
    "        timestamps = batch[\"timestamps\"][i]\n",
    "        points = batch[\"points\"][i]\n",
    "        caption = batch[\"caption\"][i]\n",
    "\n",
    "        # Determine the indices to select\n",
    "        if len(frame_idxs) <= num_frames:\n",
    "            selected_indices = list(range(len(frame_idxs)))\n",
    "        else:\n",
    "            # Select a sorted list of unique random indices to maintain temporal order\n",
    "            selected_indices = sorted(random.sample(range(len(frame_idxs)), num_frames))\n",
    "\n",
    "        # Subset the metadata based on selected indices\n",
    "        selected_frame_idxs = [frame_idxs[j] for j in selected_indices]\n",
    "        selected_timestamps = [timestamps[j] for j in selected_indices]\n",
    "        # selected_points = [points[j] for j in selected_indices]\n",
    "        # selected_points = [points[j] for j in selected_indices if j in points]\n",
    "        # print('points', points)\n",
    "        selected_points = {int(k): v for k, v in points.items() if int(k) in selected_frame_idxs}\n",
    "        # print('selected_points', selected_points)\n",
    "        \n",
    "\n",
    "        # Load images corresponding to the selected frame indices\n",
    "        images = []\n",
    "        for j in selected_indices:\n",
    "            # Construct file path: assuming images are named as \"00000.jpg\", \"00001.jpg\", etc.\n",
    "            frame_filename = f\"{frame_idxs[j]:05d}.jpg\"\n",
    "            frame_path = os.path.join(base_data_dir, video_dir, frame_filename)\n",
    "            try:\n",
    "                image = Image.open(frame_path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {frame_path}: {e}\")\n",
    "                image = None\n",
    "            images.append(image)\n",
    "\n",
    "        # Add the selected data to the new batch\n",
    "        new_batch[\"images\"].append(images)\n",
    "        new_batch[\"frame_idxs\"].append(selected_frame_idxs)\n",
    "        new_batch[\"timestamps\"].append(selected_timestamps)\n",
    "        new_batch[\"points\"].append(selected_points)\n",
    "        \n",
    "        question = SYSTEM_PROMPT.format(num_frames=num_frames, selected_frame_idxs=selected_frame_idxs) + random.choice(PROMPT_TEMPLATES).format(label=caption)\n",
    "        new_batch[\"question\"].append(question)\n",
    "        \n",
    "        answer = get_points_in_xml_format(selected_points, caption)\n",
    "        new_batch[\"answer\"].append(answer)\n",
    "\n",
    "    return new_batch\n",
    "\n",
    "# Now, apply the transform to your dataset:\n",
    "# dataset = your_huggingface_dataset.with_transform(random_augmentation)\n",
    "augmented_dataset = dataset.with_transform(random_augmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['video', 'frame_idxs', 'timestamps', 'points', 'caption', 'fps'],\n",
       "    num_rows: 572\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': [[<PIL.Image.Image image mode=RGB size=1024x1024>,\n",
       "   <PIL.Image.Image image mode=RGB size=1024x1024>,\n",
       "   <PIL.Image.Image image mode=RGB size=1024x1024>,\n",
       "   <PIL.Image.Image image mode=RGB size=1024x1024>],\n",
       "  [<PIL.Image.Image image mode=RGB size=1024x1024>,\n",
       "   <PIL.Image.Image image mode=RGB size=1024x1024>,\n",
       "   <PIL.Image.Image image mode=RGB size=1024x1024>,\n",
       "   <PIL.Image.Image image mode=RGB size=1024x1024>]],\n",
       " 'frame_idxs': [[9, 40, 57, 61], [3, 24, 26, 43]],\n",
       " 'timestamps': [[0.3, 1.3333333333333333, 1.9, 2.033333333333333],\n",
       "  [0.1, 0.8, 0.8666666666666667, 1.4333333333333333]],\n",
       " 'points': [{9: [52.0, 180.93827160493825],\n",
       "   40: [55.5, 194.37037037037035],\n",
       "   57: [54.5, 175.40740740740742],\n",
       "   61: [53.5, 168.29629629629628]},\n",
       "  {3: [52.0, 181.72839506172838],\n",
       "   24: [56.0, 186.46913580246914],\n",
       "   26: [55.75, 188.0493827160494],\n",
       "   43: [55.5, 195.1604938271605]}],\n",
       " 'question': ['You are given 4 frames from a video. The frame indices are [9, 40, 57, 61]. Object: a brown bear\\nInstruction: Point to the object.',\n",
       "  'You are given 4 frames from a video. The frame indices are [3, 24, 26, 43]. show me all visible a brown bear moving'],\n",
       " 'answer': ['<point t=\"9\" x=\"52.0\" y=\"180.9\" t=\"40\" x=\"55.5\" y=\"194.4\" t=\"57\" x=\"54.5\" y=\"175.4\" t=\"61\" x=\"53.5\" y=\"168.3\" alt=\"a brown bear\">a brown bear</point>',\n",
       "  '<point t=\"3\" x=\"52.0\" y=\"181.7\" t=\"24\" x=\"56.0\" y=\"186.5\" t=\"26\" x=\"55.8\" y=\"188.0\" t=\"43\" x=\"55.5\" y=\"195.2\" alt=\"a brown bear moving\">a brown bear moving</point>']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    batch_outputs = []\n",
    "    # Iterate over each example in the batch.\n",
    "    # for question, answer, images in zip(examples[\"question\"], examples[\"answer\"], examples[\"image\"]):\n",
    "    \n",
    "    # print('len(examples[\"question\"])', len(examples[\"question\"]))\n",
    "    # print('examples[\"question\"]', examples[\"question\"])\n",
    "    # print('examples[\"answer\"]', examples[\"answer\"])\n",
    "    \n",
    "    # print('len(examples)', len(examples))\n",
    "    # print('examples', examples)\n",
    "    \n",
    "    # for n in range(len(examples[\"question\"])):\n",
    "    for example in examples:\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"answer\"]\n",
    "        images = example[\"images\"]\n",
    "        \n",
    "        print('>>>>>>len(images)', len(images))\n",
    "        print('>>>>>>images', images[0].size) #dimensions\n",
    "        \n",
    "        # if image size is (640, 270), plot\n",
    "        if images[0].size == (640, 270):\n",
    "            plt.imshow(images[0])\n",
    "        \n",
    "        # question = examples[\"question\"][n]\n",
    "        # answer = examples[\"answer\"][n]\n",
    "        # images = examples[\"images\"][n]\n",
    "        \n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            chat_template=tokenizer.chat_template,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        if images:\n",
    "            example_inputs = processor.process(\n",
    "                images=images,\n",
    "                text=prompt,\n",
    "                for_training=True\n",
    "            )\n",
    "        else:\n",
    "            example_inputs = processor.process(\n",
    "                text=prompt,\n",
    "                for_training=True\n",
    "            )\n",
    "        \n",
    "        # Move to cuda and add a batch dimension.\n",
    "        example_inputs = {k: v.to(\"cuda\").unsqueeze(0) for k, v in example_inputs.items()}\n",
    "        \n",
    "        batch_outputs.append(example_inputs)\n",
    "    \n",
    "    # Now, collate the list of dictionaries into a single batch dictionary.\n",
    "    # (Assuming each key in example_inputs is a tensor of the same shape across examples.)\n",
    "    batch_inputs = {}\n",
    "    for key in batch_outputs[0]:\n",
    "        # print('>>> key', key)\n",
    "        # for bt in batch_outputs:\n",
    "        #     print('bt[key].shape', bt[key].shape)\n",
    "        batch_inputs[key] = [ex[key] for ex in batch_outputs] #torch.cat([ex[key] for ex in batch_outputs], dim=0)\n",
    "    \n",
    "    # return batch_inputs\n",
    "    \n",
    "    '''\n",
    "    batch_inputs:\n",
    "    \n",
    "        input_ids torch.Size([B, 1249])\n",
    "        images torch.Size([B, 13, 576, 588])\n",
    "        image_input_idx torch.Size([B, 13, 144])\n",
    "        image_masks torch.Size([B, 13, 576])\n",
    "        labels torch.Size([B, 1249])\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # batch_inputs has keys: input_ids', 'images', 'image_input_idx', 'image_masks', 'labels'\n",
    "    \n",
    "    # print('>> batch_inputs', batch_inputs)\n",
    "    \n",
    "    \n",
    "    # padded_inputs = tokenizer.pad({\"input_ids\": [ex[\"input_ids\"][0] for ex in batch_inputs],},\n",
    "    #     padding=True, return_tensors=\"pt\",\n",
    "    # )\n",
    "    \n",
    "    padded_inputs = tokenizer.pad({\"input_ids\": [ex[0] for ex in batch_inputs['input_ids']],},\n",
    "        padding=True, return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Create the labels and set the special tokens to -100\n",
    "    labels = padded_inputs[\"input_ids\"].clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Set the special tokens to -100\n",
    "    labels[labels == processor.special_token_ids[IMAGE_PROMPT]] = -100\n",
    "    labels[labels == processor.special_token_ids[DEFAULT_IMAGE_PATCH_TOKEN]] = -100\n",
    "    labels[labels == processor.special_token_ids[DEFAULT_IM_START_TOKEN]] = -100\n",
    "    labels[labels == processor.special_token_ids[DEFAULT_IM_END_TOKEN]] = -100\n",
    "    labels[labels == processor.special_token_ids[DEFAULT_IM_COL_TOKEN]] = -100\n",
    "    \n",
    "    padded_inputs[\"labels\"] = labels\n",
    "    \n",
    "    # padded_inputs[\"images\"] = torch.cat([example[\"images\"] for example in batch_inputs], 0)\n",
    "    # padded_inputs[\"image_input_idx\"] = torch.cat([example[\"image_input_idx\"] for example in batch_inputs], 0)\n",
    "    # padded_inputs[\"image_masks\"] = torch.cat([example[\"image_masks\"] for example in batch_inputs], 0)\n",
    "    \n",
    "    for ex in batch_inputs['images']:\n",
    "        print('ex.shape', ex.shape)\n",
    "    \n",
    "    padded_inputs[\"images\"] = torch.cat([example for example in batch_inputs[\"images\"]], 0)\n",
    "    padded_inputs[\"image_input_idx\"] = torch.cat([example for example in batch_inputs[\"image_input_idx\"]], 0)\n",
    "    padded_inputs[\"image_masks\"] = torch.cat([example for example in batch_inputs[\"image_masks\"]], 0)\n",
    "    \n",
    "    padded_inputs = padded_inputs.to(torch.bfloat16).to(\"cuda\")\n",
    "    \n",
    "    return padded_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['LD_LIBRARY_PATH'] = \"/share/softwares/cuda_cudnn/cuda-12.1/lib64:/lib/x86_64-linux-gnu:\" + os.environ.get('LD_LIBRARY_PATH', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from molmo.model import MolmoForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"allenai/Molmo-7B-D-0924\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb90c74ab3747e2bf74f95e4e4236a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "USE_QLORA = True\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# load the model\n",
    "if USE_QLORA:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model = MolmoForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "else:\n",
    "    model = MolmoForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,124,736 || all params: 8,032,150,016 || trainable%: 0.1385\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration. By setting target_modules to [\"att_proj\", \"ff_proj\"],\n",
    "# only the transformer (LLM) portion will have LoRA adapters.\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",  # for causal language modeling\n",
    "    r=8,                    # LoRA rank (adjust as needed)\n",
    "    lora_alpha=8,          # scaling factor\n",
    "    lora_dropout=0.1,       # dropout probability for LoRA layers\n",
    "    target_modules=[\"att_proj\", \"ff_proj\"]\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA (this only affects modules matching the target names)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Optional: print out trainable parameters to verify that only the LLM part is modified.\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 8\n",
    "\n",
    "ds_config = {\n",
    "    \"zero_optimization\": {\n",
    "      \"stage\": 3,\n",
    "      \"offload_param\": {\n",
    "        \"device\": \"cpu\"\n",
    "      },\n",
    "      \"offload_optimizer\": {\n",
    "        \"device\": \"cpu\"\n",
    "      }\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "    \n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \n",
    "    \"bf16\": {\n",
    "      \"enabled\": True\n",
    "    }\n",
    "  }\n",
    "\n",
    "# from transformers import TrainingArguments\n",
    "# training_args=TrainingArguments(\n",
    "#             num_train_epochs=10,\n",
    "#             remove_unused_columns=False,\n",
    "#             per_device_train_batch_size=4,\n",
    "#             gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#             warmup_steps=1,\n",
    "#             learning_rate=2e-5,\n",
    "#             weight_decay=1e-6,\n",
    "#             adam_beta2=0.999,\n",
    "\n",
    "#             save_strategy=\"no\", #TODO: change to 'steps' or 'epoch'\n",
    "#             # optim=\"adamw_hf\",\n",
    "#             optim=\"adamw_torch\",\n",
    "#             push_to_hub=True,\n",
    "#             save_total_limit=1,\n",
    "#             bf16=True,\n",
    "#             output_dir=\"./lora-molmo-pixmo-video\",\n",
    "            \n",
    "#             logging_strategy=\"steps\",\n",
    "#             report_to=\"wandb\",\n",
    "#             logging_steps=10,\n",
    "            \n",
    "#             dataloader_pin_memory=False, \n",
    "            \n",
    "#             deepspeed=ds_config,\n",
    "#             # distributed_port=29501, #TODO\n",
    "        # )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"molmo-video\",  # Directory to save the model\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # Steps to accumulate gradients\n",
    "    \n",
    "    gradient_checkpointing=False,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=10,  # Steps interval for evaluation\n",
    "    \n",
    "    eval_strategy= \"no\", # \"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"no\", #steps\",  # Strategy for saving the model\n",
    "    \n",
    "    save_steps=20,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=True,  # Whether to push model to Hugging Face Hub\n",
    "    report_to=\"wandb\",  # Reporting tool for tracking metrics\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    #max_seq_length=1024  # Maximum sequence length for input\n",
    "    \n",
    "    deepspeed=ds_config,  # DeepSpeed configuration\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3668347/2984811085.py:13: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-24 21:17:34,196] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# from transformers import Trainer\n",
    "# trainer = Trainer(\n",
    "#         model=model,\n",
    "#         train_dataset=augmented_dataset,\n",
    "#         data_collator=collate_fn,\n",
    "#         args=training_args,\n",
    "#         )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=augmented_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/shehan/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/shehan/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.580343246459961 seconds\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The server socket has failed to listen on any local network address. port: 29500, useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/transformers/trainer.py:2155\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2162\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/transformers/trainer.py:2322\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2320\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   2321\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2322\u001b[0m         model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[1;32m   2325\u001b[0m     model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m   2326\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[1;32m   2327\u001b[0m     )\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/accelerate/accelerator.py:1383\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_ao(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m-> 1383\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_deepspeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mMEGATRON_LM:\n\u001b[1;32m   1385\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_megatron_lm(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/accelerate/accelerator.py:1924\u001b[0m, in \u001b[0;36mAccelerator._prepare_deepspeed\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1921\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(scheduler)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m deepspeed\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mlr_schedules\u001b[38;5;241m.\u001b[39mVALID_LR_SCHEDULES:\n\u001b[1;32m   1922\u001b[0m                 kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m scheduler\n\u001b[0;32m-> 1924\u001b[0m engine, optimizer, _, lr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mds_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compare_versions(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepspeed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.14.4\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdynamo_plugin\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m!=\u001b[39m DynamoBackend\u001b[38;5;241m.\u001b[39mNO:\n\u001b[1;32m   1926\u001b[0m     compile_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdynamo_plugin\u001b[38;5;241m.\u001b[39mto_kwargs()\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/deepspeed/__init__.py:144\u001b[0m, in \u001b[0;36minitialize\u001b[0;34m(args, model, optimizer, model_parameters, training_data, lr_scheduler, distributed_port, mpu, dist_init_required, collate_fn, config, mesh_param, config_params)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m comm \u001b[38;5;28;01mas\u001b[39;00m dist\n\u001b[1;32m    143\u001b[0m dist_backend \u001b[38;5;241m=\u001b[39m get_accelerator()\u001b[38;5;241m.\u001b[39mcommunication_backend_name()\n\u001b[0;32m--> 144\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_distributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdistributed_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdist_init_required\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_init_required\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m##TODO: combine reuse mpu as mesh device and vice versa\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Set config using config_params for backwards compat\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m config_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/deepspeed/comm/comm.py:685\u001b[0m, in \u001b[0;36minit_distributed\u001b[0;34m(dist_backend, auto_mpi_discovery, distributed_port, verbose, timeout, init_method, dist_init_required, config, rank, world_size)\u001b[0m\n\u001b[1;32m    683\u001b[0m     utils\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitializing TorchBackend in DeepSpeed with backend \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dist_backend))\n\u001b[1;32m    684\u001b[0m \u001b[38;5;66;03m# Create a torch backend object, initialize torch distributed, and assign to cdb\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m cdb \u001b[38;5;241m=\u001b[39m \u001b[43mTorchBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_backend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/deepspeed/comm/torch.py:118\u001b[0m, in \u001b[0;36mTorchBackend.__init__\u001b[0;34m(self, backend, timeout, init_method, rank, world_size, name)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Future functionality to support ds.initialize() on a single GPU\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# The idea is to fake that dist backend is initialized even when\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# it is not so we can run on a single GPU without doing any init_process_group\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_gpu_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshm_comm_op \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshm_comm_op\u001b[38;5;241m.\u001b[39minitialize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_world_size(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rank())\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/deepspeed/comm/torch.py:148\u001b[0m, in \u001b[0;36mTorchBackend.init_process_group\u001b[0;34m(self, backend, timeout, init_method, rank, world_size)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minit_process_group\u001b[39m(\u001b[38;5;28mself\u001b[39m, backend, timeout, init_method, rank, world_size):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[0;32m--> 148\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43minit_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_mpi \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmpi\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     85\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:97\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     96\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[0;32m---> 97\u001b[0m     func_return \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m t1\n\u001b[1;32m    100\u001b[0m     msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:1520\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1517\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[1;32m   1518\u001b[0m         not_none(init_method), rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1519\u001b[0m     )\n\u001b[0;32m-> 1520\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/rendezvous.py:269\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m master_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(_get_env_or_raise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    267\u001b[0m use_libuv \u001b[38;5;241m=\u001b[39m _get_use_libuv_from_query_dict(query_dict)\n\u001b[0;32m--> 269\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c10d_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaster_addr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_libuv\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m (store, rank, world_size)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# If this configuration is invalidated, there is nothing we can do about it\u001b[39;00m\n",
      "File \u001b[0;32m/share/softwares/anaconda/anaconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/rendezvous.py:189\u001b[0m, in \u001b[0;36m_create_c10d_store\u001b[0;34m(hostname, port, rank, world_size, timeout, use_libuv)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     start_daemon \u001b[38;5;241m=\u001b[39m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTCPStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_daemon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_tenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_libuv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_libuv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The server socket has failed to listen on any local network address. port: 29500, useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"path_to_save_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
